[
    {
        "Published": "2024-02-08",
        "Title": "Choose Your Weapon: Survival Strategies for Depressed AI Academics",
        "Authors": "Julian Togelius, Georgios N. Yannakakis",
        "Summary": "Are you an AI researcher at an academic institution? Are you anxious you are\nnot coping with the current pace of AI advancements? Do you feel you have no\n(or very limited) access to the computational and human resources required for\nan AI research breakthrough? You are not alone; we feel the same way. A growing\nnumber of AI academics can no longer find the means and resources to compete at\na global scale. This is a somewhat recent phenomenon, but an accelerating one,\nwith private actors investing enormous compute resources into cutting edge AI\nresearch. Here, we discuss what you can do to stay competitive while remaining\nan academic. We also briefly discuss what universities and the private sector\ncould do improve the situation, if they are so inclined. This is not an\nexhaustive list of strategies, and you may not agree with all of them, but it\nserves to start a discussion.",
        "Page_content": "1\nChoose Your Weapon:\nSurvival Strategies for Depressed AI Academics\nJulian Togelius, Senior Member, IEEE, Georgios N. Yannakakis, Fellow, IEEE\nI. INTRODUCTION\nA\nS someone who does Artificial Intelligence (AI) research\nin a university, you develop a complicated relationship\nto the corporate AI research powerhouses, such as Googe\nDeepMind, OpenAI, and Meta AI. Whenever you see one\nof these papers that train some kind of gigantic neural net\nmodel to do something you were not even sure a neural\nnetwork could do, unquestionably pushing the state of the\nart and reconfiguring your ideas of what is possible, you get\nconflicting emotions. On the one hand: it is very impressive.\nGood on you for pushing AI forward. On the other hand:\nhow could we possibly keep up? As an AI academic, leading\na lab with a few PhD students and (if you’re lucky) some\npostdoctoral fellows, perhaps with a few dozen Graphics\nProcessing Units (GPUs) in your lab, this kind of research\nis simply not possible to do.\nTo be clear, this was not always the case. As recently as\nten years ago, if you had a decent desktop computer and an\ninternet connection you had everything you needed to compete\nwith the best of researchers out there. Ground-breaking papers\nwere often written by one or two people who ran all the\nexperiments on their regular workstations. It is useful to point\nthis out particularly for those who have come into the research\nfield within the last decade, and for which the need for gigantic\ncompute resources is a given.\nIf we have learned one thing from deep learning [9], it\nis that scaling works. From the ImageNet [19] competitions\nand their various winners to ChatGPT, Gato [17], and most\nrecently to GPT-4 [1], we have seen that more data and more\ncompute yield quantitatively and often even qualitatively better\nresults. (By the time you are reading this, that list of very\nrecent AI milestones might very well be outdated.). Of course\nthere are improvements to learning algorithms and network\narchitectures as well, but these improvements are mostly useful\nin the context of the massive scale of experiments. (Sutton\ntalks about the “Bitter Pill”, referring to the insight that\nsimple methods that scale well always win the day when\nmore compute becomes available [22].) A scale that is not\nachievable by academic researchers nowadays. As far as we\ncan tell, the gap between the amount of compute available\nto ordinary researchers and the amount available to stay\ncompetitive is growing every year.\nThis goes a long way to explain the resentment that many AI\nresearchers in academia feel towards these companies. Healthy\nJT is with New York University, GNY is with University of Malta. The\nword “depressed” in the title does not refer to either the clinical or economic\nconcept of depression, but rather the word’s everyday use as signifying an\nunhappy and/or hopeless state of mind.\ncompetition from your peers is one thing, but competition from\nsomeone that has so much resources that they can easily do\nthings you could never do, no matter how good your ideas are,\nis another thing. When you have been working on a research\ntopic for a while and, say, DeepMind or OpenAI decides to\nwork on the same thing, you will likely feel the same way as\nthe owner of a small-town general store feels when Walmart\nsets up shop next door. Which is sad, because we want to\nbelieve in research as an open and collaborative endeavor\nwhere everybody gets their contribution recognized, don’t we?\nSo, if you are but a Professor, with a limited team size and\nlimited compute resources, what can you do to stay relevant\nin face of the onslaught of incredibly well-funded research\ncompanies? This is a question that has been troubling us\nand many of our colleagues for years now. Recent events,\nwith models such as GPT-4 being shockingly capable and\nshockingly closed-sourced and devoid of published details, has\nmade the question even more urgent. We have heard from\nmultiple researchers at various levels of seniority, both in-\nperson and via social media, who worry about the prospects\nof doing meaningful research given the lack of resources and\nthe unfair competition from big tech companies.\nLet us make this clear at the outset: both of us are secure.\nWe hold tenured academic Professorships and we rose up on\nthe academic ladder pretty fast, in part because of finding\nan academic niche: we systematically pushed the envelope of\nAI in the domain of video games. While we obviously care\nabout continuing to do relevant AI research ourselves, we are\nwriting this mostly for our more junior colleagues, postdocs\nand doctoral students, who may wonder about which career\npath to choose. Is it worthwhile to go into academia, or is it\nbetter to join a big tech company, or maybe kick off a startup?\nIs a career in AI a good idea, or is it better to become a\nplumber? Should you be a cog in the machinery, or a rebel?\n(It’s usually easier to be a rebel when you have nothing to lose,\nwhich is either at the beginning of your career or when you\nhave tenure.) As skilled as one may be, is this glorious battle\nto stay competitive lost already? Are we about to lie here,\nobedient to our laws? This Point of View article is partly meant\nas serious advice, and partly as emotional encouragement, but\nperhaps most of all to start a discussion with all of you so we\nimprove our position as academics before the battle is long\nlost. We do not wish to stop the evolution of AI technology\n(even if we could); quite the contrary: we wish to discuss the\nstrategies that will equip as many as possible to be part of this\njourney. While the challenges are real and many, we both feel\nthe are even more opportunities and the time is right to grab\nthem!\narXiv:2304.06035v2  [cs.OH]  8 Feb 2024\n2\nIn the remainder of this article we list a number of ideas\n(or strategies) for what to do if you are an AI academic\ndespairing about your options. These options are presented\nin no particular order. We also don’t make any particular\nrecommendations here or ranking the options for you. It is up\nto you to pick one, more than one, or none of them as your\nfavourite direction. Towards the end of the article, however,\nwe discuss what big tech companies and universities can do to\nhelp the situation. There, we make some specific suggestions.\nII. GIVE UP!\nGiving up is always an option. Not giving up on doing\nresearch, but giving up on doing things that are really im-\npactful and pushing the envelope. There are still plenty of\ntechnical details and sub-sub-questions to publish papers about\nin mid-tier journals and conferences. Please note, however:\n(1) This works best if you already have a secure permanent\nposition and you do not care much about promotions, (2) this\nwasn’t really what you dreamed of doing when you decided\non a research career, right? Forcing yourself to reframe your\nresearch agenda because of this fierce competition is similar to\nadjusting your research to the priorities of funding bodies like\nthe European Commission or US National Science Foundation.\nAt least going for the latter might secure some funding for\nyour lab which can, in turn, help you work with some talented\nAI researchers and doctoral students. It is important to note\nthat we both consider ourselves lucky enough as we have\ncoordinated or have been part of several small- and large-\nscale research projects1 that allowed us to support our research\nagendas and helped us (in part) to secure our positions.\nIII. TRY SCALING ANYWAY\nGoing head-to-head with an overwhelming competition is\nan admirable sentiment. If scaling works, let’s do it in our\nuniversity labs! Let’s go tilting at windmills (GPU fans)!\nThe most obvious problem is access to central processing\nunits (CPUs) and GPUs. So, let’s say you secure $50k of\nfunding for cloud compute from somewhere and go ahead\nrunning your big experiment. But this is a very small amount\nof money compared to what training something like GPT-3\ncosts. The recent open AI agent that learned to craft a diamond\npickaxe in Minecraft required training of 9 days on 720 V100\nGPUs [2]; this amounts to a few hundred thousand dollars for\na single experiment. Not even prestigious European Research\nCouncil (EU) or National Science Foundation (US) grants can\nsupport such a level of investment. Still, spending $50k on\ncloud compute will give you significantly more compute than\na bunch of gaming PCs taped together, so you could scale at\nleast a little bit. At least for that very experiment. But as we\nall know, most experiments don’t work the first time you try\nthem. For every big successful experiment we see reported, we\nhave unreported months or maybe years of prototypes, proofs\nof concept, debugging, parameter tuning, and failed starts. You\nneed this level of compute available constantly.\n1Examples include the H2020 AI4Media (https://www.ai4media.eu/) and\nthe FP7 C2Learn (http://project.c2learn.eu/) projects.\nThe less obvious problem is that you need the right kind\nof team to build experimental software that scales, and that\nis generally not compatible with academic career structures.\nMost of the members of a typical academic research lab\nin computer science are PhD students that need to graduate\nwithin a few years, and need to have an individual project to\nwork on which results in multiple first-author papers so they\ncan get a job afterwards. A large-scale AI project typically\nmeans that most members of the team work for many months\nor years on the same project, where only one of them can\nbe the first author on the paper. The team will probably also\ninclude people who do “mundane” software engineering tasks\nthat are crucial to the success of the project, but which are not\nseen as AI research in themselves. The structures needed for\nsuccessful large scale projects are simply not compatible with\nthe structures of academia.\nIV. SCALE DOWN\nOne popular way to bypass the issue is to focus on simple\nyet representative (toy) problems that will either prove the\nbenefits of a new approach theoretically or showcase the\ncomparative advantages of a novel method. Indicatively, a\nrecent paper on Behaviour Transformers [21] showcased the\nbenefits of the method on a toy navigation task that only took a\nsimple multi layer perceptron to solve. A similar approach was\nlater used in [15]. Both studies will likely be impactful despite\nthe limited scale because they demonstrated the capacity of the\nalgorithms in popular game and robotic benchmark problems\nthat require large models and significant compute to train. In\n[14] we observe the same pattern once again: a case is made\nin a toy (gambling) environment but the impact, one would\nargue, comes from the comparative advantages the algorithm\nshows in more complex but computationally heavy problems.\nA downside with this approach is that people are wowed by\npretty colors in high resolution, and take a real car navigating a\nroad more seriously than a toy car, even though the challenges\nmay be the same. So you will get less media exposure, perhaps\nless funding. There are also domains, such as language, which\nare very hard to scale down beyond some limit.\nV. REUSE AND REMASTER\nA key reason that AI has advanced so rapidly over the\nlast decade is that researchers make their code and models\navailable to the scientific community. Model sharing and code\naccessibility was neither the norm nor the priority of AI\nresearchers back in the days. Having access to pretrained large\nmodels like ViT in vision [4] or the Llama family for text [25]\nsaves you time and effort as you can simply resue them,\nand fine-tune them for your own specific problem. Arguably,\none needs to assume that the representations of those large\nmodels is general enough to be able to perform well to your\ndownstream task with limited training. Unfortunately the fine-\ntuning and post-hoc analysis of a large model is sometimes\nnot enough for good performance, especially if your domain is\nquite different from what they were pre-trained for. Relying on\npre-trained models is therefore limiting the scope of research\nyou can do.\n3\nVI. ANALYSIS INSTEAD OF SYNTHESIS\nAnother thing one can do with the publicly available pre-\ntrained models is to analyze them. While this may not directly\ncontribute to new capabilities, it can still make scientific\nprogress. The current state of things is that we have great\nmodels for text and image generation publicly available, but\nwe don’t understand them very well. You could even argue that\nwe barely understand them all. Let’s face it: a transformer\nis not an intuitive thing to anyone, and the scale of data\nthese models are trained on is almost incomprehensible in\nitself. There is plenty of work to do in analyzing them, for\nexample by probing them in creative ways, and developing\nvisualizations and conceptual machinery to help us understand\nthem.\nOne can do analysis with different mindsets. Trying to find\nand describe specific circuits and mechanisms that have been\nlearnt is useful, and can help us (well, someone else, with\nresources) to create better models in the future. But one can\nalso play the role of the gadfly, incessantly finding ways to\nbreak them! This is scientifically and societally valuable, no\nmatter what those who try to make a business out of large\nmodels say. But it might not be the kind of research you want\nto do.\nVII. RL! NO DATA!\nOne might scale down one’s requirements with respect to\ndata and instead approach AI problems through the lens of\n(online) reinforcement learning (RL). Following the RL path\nmight allow you to bypass issues related to data availability,\nanalysis, storage and handling; it does not however minimize\nthe computational effort required necessarily. In fact, even the\nmost efficient RL methods are known to be computationally\nheavy as the very process of exploration is costly. Moreover,\nshaping a reward function often involves forms of black art\n(informally) or practical wisdom (more formally). That is, a\nresearcher often needs to continuously run lengthy experiments\nwith different types of reward (among other hyperparameters)\nfor a breakthrough result. So ultimately one has to downscale\nthe complexity of the problem once again. The bottom line\nis that if you want to break free from large data sets you\nmight be still faced with large compute requirements unless\nyou work on simple (toy) problems, specialized domains, or\nwork with small models; the next section is dedicated to the\nlatter strategy.\nVIII. SMALL MODELS! NO COMPUTE!\nAnother valid strategy is to compromise on model scale to\nsave on compute. There are many circumstances where you\nwant or need a smaller model. Think of the smallest possible\nmodels that are capable of solving a problem or completing\na task. This is particularly important to and relevant for\nreal-world applications. In-the-wild domains such as games,\ninternet of things, and autonomous vehicles could allow AI\nto be deployed next to their end user and the data the user\ngenerates, i.e. at the edge of the network. This is often called\nedge AI [10], the operation of AI applications in devices of\nthe physical world is possible when memory requirements are\nlow and inference occurs rapidly. Neuroevolution and neural\narchitecture search [10], and knowledge distillation [6], [13]\nmethods are only a few of the available methods for edge\nAI. Note that beyond learning more from smaller models one\ncould also attempt to learn more from less data [7]. Following\nthis research path may lead to significant into models’ inner\nworkings. Studying small AI models makes the analysis far\neasier and increases the explainability of whatever the model\ndoes. Moreover, deploying such models on devices helps\nwith privacy concerns. You can also argue for small models\nfrom the perspective of green AI [20] , as it minimizes the\nenvironmental footprint of the research. Obviously there are\nlimits to what a small model is capable of doing but the\nimportance of this research direction, we feel, will be growing\ndrastically over the years.\nIX. WORK ON SPECIALIZED APPLICATION AREAS OR\nDOMAINS\nOne rather efficient strategy is to pick a niche but some-\nwhat established area of research—-that is likely beyond the\nimmediate interest of the industry—and try to innovate within\nand through that area. It is often a successful strategy to\nbring and test your ideas to an entirely new domain but\nit is less often that the outcomes will have a large impact\nbeyond that domain. There are plenty of examples of niche\nareas eventually becoming dominant due to the push of a few\ndedicated researchers. We are both currently mostly taking this\nstrategy: we have the AI for games community as primary\nscientific community where we can perform state-of-art work,\nas few large companies put serious efforts into modern AI for\ngames.\nThink of video games as a domain that penetrated the\nresearch communities of robotics and computer vision back\nin early 00s, and again with video games as deep RL bench-\nmarks after 2015. Think of neural networks and deep learning\nmethods that came to dominate communities invested in\nsupport vector machines and regression models (e.g. NeurIPS\na decade ago). Also think of the ways reinforcement learning\nand deep learning have altered the core principles of multi-\nagent learning and cognitive/affect modeling in communities\nrepresented by the AAMAS, ACII and IVA conferences, for\ninstance.\nA core downside to this strategy is the difficulty getting\nyour paper accepted in the kind of large venues that are most\ninfluential in AI, such as NeurIPS, AAAI, ICML and IJCAI.\nYour paper and its results might end up sitting out-of-the-\ninterest-distribution. It is, however, very possible to start your\nown community with its own publication venues.\nIf you do not have the requisite domain expertise—and/or\ndatasets—yourself, you can fruitfully approach domain experts\nto collaborate. The good news is that as an academic, you have\nplenty of such experts in other departments of your university\nor institute and they all have interesting AI problems to solve\nif you spend some time talking to them. One of the authors\nrecently ran in to an anthropologist and an analytical chemist\nin a corridor, and started discussing projects that would include\nall three. Another example is a recent collaboration of one of\n4\nthe authors with urban designers resulting in the reconstruction\nof urban areas around MIT and Harvard for improving the\ncomfort levels of Bostonians [5].\nThese projects may not end up advancing the state of\nAI much, but may make big differences in the particular\ndisciplines. And sometimes big AI advances come from\napplication-specific work.\nX. SOLVE PROBLEMS FEW CARE ABOUT (FOR NOW!)\nWhile focusing on an established niche or application field\nis a relatively safe strategy, a somewhat riskier one is to find\na niche or application that does not exist yet. Basically, focus\non a problem that almost no-one sees the importance of, or a\nmethod that nobody finds promising.\nOne approach is to go looking for applications that people\nhave not seriously applied AI to. A good idea is to look into a\nfield that is neither timely nor “sexy”. The bet here is that this\nparticular application domain will become important in the\nfuture, either in its own right or because it enables something\nelse. We both took this path. Procedural content generation\nfor games was a very niche topic 15 years ago and we helped\nnuild a research community around it [24], [28]; recently it has\nbecome more important not only for the games industry, but\nalso as a way to help generalize (deep) reinforcement learning\n[18], [23]. Research on reinforcement learning is a core AI\ntopic with thousands of papers published per year, lending\nmore importance to this once somewhat obscure topic. This\nhigh-risk high-gain mindset might lead to a lonely path that\nnevertheless could end up being highly rewarding in the long\nrun.\nSo, look around you, and talk to people who are not AI\nresearchers. What problem domains do you see where AI is\nrarely applied, and which AI researchers seem to not know\nor care about? Might someone care about these domains in\nthe future? If so, you may want to dig deeper in one of those\ndomains.\nXI. TRY THINGS THAT SHOULDN’T WORK\nAnother comparative advantage of small academic teams is\nthe ability to try things that “shouldn’t work”, in the sense that\nthey are unsupported by theory or experimental evidence. The\ndynamics of large industry research labs are typically such\nthat researchers are incentivized to try things that are likely\nto work; if not, money is lost. In academia, failure can be as\ninstructive and valuable as success and the stakes are lower\noverall. Many important inventions and ideas in AI come from\ntrying the “wrong” thing. In particular, all of deep learning\nstems from researchers stubbornly working on neural networks\neven though there were good theoretical reasons why they\nshouldn’t work.\nXII. DO THINGS THAT HAVE BAD OPTICS\nThe larger and more important a company is, the more con-\nstrained it is by ethics and optics. Any company is ultimately\nresponsible to their shareholders, and if the shareholders\nperceive that the company suffers “reputational damage” they\ncan easily fire the CEO. So large companies will try to avoid to\ndo anything that looks bad. To get around this, large companies\nsometimes fund startups to do their more experimental work\nthat might go wrong (think Microsoft and OpenAI). But even\nsuch plays have limits, as bad PR can come washing back like\nthe tide in San Francisco Bay.\nAs an individual researcher, who either has no position or\nwho already has a secure position, you have nothing to lose.\nYou can do things that are as crazy as you like. You are only\nconstrained by the law and your own personality. Now, we are\nin no way arguing that you should do research that is unethical.\nBy all means, try to do the right thing. But what you find\nobjectionable might be very different from what a group of\nmostly-white liberal overeducated engineers in coastal USA\nfind objectionable. The PR departments, ethics committees,\nand boards of directors of the rich tech companies espouse\na very particular set of values. But the world is large, and\nfull of very different people and cultures. So there is a big\nopportunity to do research that these tech companies will not\ndo even though they could.\nAs an example of a project that exploits such an opportunity,\none of us participated in a project critically examining the\nnormativity of the “neutral English” in current writing support\nsystems by creating an autocomplete system with a language\nmodel that assumes you write in the tone of Chuck Tingle, the\nfamous author of absurd sci-fi political satire gay erotica [8].\nOur guess is that this project would not have been cleared for\npublication by Amazon or Google. Another example is this\nvery paper.\nSimilarly, you may find that you deviate from the cultural\nconsensus in big tech companies regarding topics relating to\nnudity, sexuality, rudeness, religion, capitalism, communism,\nlaw and order, justice, equality, welfare, representation, history,\nreproduction, violence, or something else. As all AI research\nhappens in and is influenced by a cultural and political context,\nsee your deviation from the norm as an opportunity. If you\ncan’t do the research they couldn’t do, do the research they\nwouldn’t do.\nXIII. START IT UP; SPIN IT OUT!\nBy now it should be rather clear that academia is somewhat,\nparadoxically, limiting academic AI research. Even if one\nmanages to secure large-scale multimillion projects this covers\nonly a fraction of human and computational resources that\nare necessary for contemporary AI research, and the career\nstructures and IP rights regimes of universities often impose\nfurther limits. One popular alternative among AI scientists is\nto spin out their idea from their university lab and found a\ncompany that will gradually transfer AI research to a set of\ncommercial-standard services or products. Both authors have\nbeen part of this journey through co-founding modl.ai [16] and\nhave learned a lot from this.\nBeing part of the applied AI world offers many benefits. In\nprinciple you get access to rich data from real-world applica-\ntions that you wouldn’t be able to have otherwise. Moreover\nyour AI algorithms are tested on challenging commercial-\nstandard, applications and have to be operational in the wild.\nFinally, you usually gain access to more compute and, if the\nstart-up scales up, growing access to human resources.\n5\nThis journey is far from straightforward, however, as there\nare several limiting factors to consider. First, not all research\nideas are directly applicable to a startup business model. Your\nbest research ideas might be brilliant in terms of understanding\nthe world, or at least getting published in highly prestigious\nvenues, but that does not mean that one can easily make\nproducts out of them. Second, many outstanding results one\nobtained in the lab today may have to go through a long\nrunway until they turn into a business case of some sort. Most\nstartups do development rather than research, as the runways\nare short and you need to have a functioning product, prefer-\nably with some market traction, before the next funding round\nin two years or so. Third, even if you do get some investment,\nthis does not mean you have an unlimited compute budget.\nWith seed grants often in the range of a few millions, this\ndoes not buy you the capacity to do OpenAI-level experiments,\nespecially as you need to pay real salaries (not PhD stipends)\nto your employees. Fourth, not every AI academic enjoys this\ntype of an adventure. At the end of the day most academics\nhave long agreed on their priorities when they opted to follow\nthe academic career path. You don’t become a professor for\nthe money. The security of an academic environment (given\nthat it is both safe and creative), means to some far more than\nany potentially higher salary or other corporate benefits.\nHere, we might point out that both of us publish many\nmore papers with our academic research teams than with the\ncompany we co-founded and work part-time at. On the other\nhand, we believe we have more direct impact on the games\nindustry through our company.\nXIV. COLLABORATE, OR JUMP SHIP!\nIf none of the above options work for you and you still\nwant to innovate though large scale methods that are trained\non lots of data you can always collaborate with those that have\nthem both: compute and data. There are several ways to move\nforward with this approach.\nUniversities in the vicinity of leading AI companies have\na comparative advantage as local social networks and in-\nperson meetings make the collaboration easier. Researchers\nfrom remote universities can still establish collaborations\nthough research visits, placements and internships as part of\na joint-research project. More radically, some established AI\nprofessors decide to dedicate some (if not all) of their research\ntime to an industrial partner or even move their entire lab\nin there. Results from such partnerships, placements or lab\ntransfers can be astonishing [26], [27]. At a glance, this looks\nlike the best way forward for AI academics, however, 1) the\ngenerated IP cannot always be published and 2) not everyone\ncan or want to work in an industry-based AI lab.\nOne might even argue that innovation should be driven by\npublic institutions as supported by the industry, not the other\nway around. It is arguably the university’s responsibility to\nmaintain (part of, or some of) the talented AI researchers it\neducates (academics and students) and the IP they generate.\nOtherwise AI education and research will eventually become\nredundant within a University environment. This would be bad\nfor everyone, as knowledge would be less open, and there\nwould be no-one to train the next generation of AI researchers.\nNext, let’s look at this relationship more closely and outline\nways industrial corporations and universities may be able to\nhelp.\nXV. HOW CAN LARGE PLAYERS IN INDUSTRY HELP?\nIt is not clear that large companies with well-financed AI\nlabs actually want to help alleviate this situation. Individual\nresearchers and managers might care about the depression of\nacademic AI research, but what the companies care about is the\nbottom line and shareholder value, and having a competitive\nacademic research community might or might not be in their\nbest interest. However, to the extent that large private sector\nactors do care, there are multiple things they can do.\nAt the most basic level, open-sourcing models, including\nboth weights and training scripts, helps a lot. It allows aca-\ndemic AI researchers to study the trained models, fine-tune\nthem, and build systems around them. It still leaves academic\nresearchers uncompetitive when it comes to training new\nmodels, but it is a start. To their credit, several large industrial\nresearch organizations regularly release their most capable\nmodels publicly; Meta in particular stands out. Others don’t,\nand could rightly be shamed for not doing so. In particular if\ntheir name implies some degree of openness.\nThe next step for remedying this situation is to collaborate\nwith academia. As discussed earlier (see Section XIV) some\nlarge institutions regularly do this, mostly through accepting\ncurrent PhD students as interns, allowing these students to do\nlarge-scale work. Some offer joint appointments to certain aca-\ndemic researchers, and a few even occasionally offer research\ngrants. All of this is good, but more can be done. In particular\nthere could be mechanisms where academics initiate collabo-\nrations by proposing work they would do collaboratively, and\nthere could be more stable research funding mechanisms.\nGoing even further, private companies that really wanted\nto help mend this academia-industry divide could choose to\nwork in public: post their plans, commit code, models, and\ndevelopment updates to public repositories and allowing aca-\ndemics to contribute freely. This is not how most companies\nwork, and often they have good reasons for their secrecy. On\nthe other hand, a lot could be gained from having academics\ncontributing to your code and training for free.\nXVI. HOW CAN UNIVERSITIES HELP?\nAs much as industry might be willing to help, the primary\ninitiative should come from those universities that wish to\ndrive innovation. Universities have a strong initiative to stay\non top of (or if possible be in the driving seat for) AI research\nfor many reasons, including their role in educating students\nwho will look for jobs in a world transformed by AI, and the\nmany ways in which AI systems transform education [12]. It\nis worth noting that many of the most influential papers in AI\ninvolve a university department. Those papers are typically\nco-authored by researchers that either collaborate with or are\ninvolved in a company. The successful examples are out there\n[3], [26], [27], but more is needed from the university’s end to\nenable such partnerships. And actually, there are many ways\n6\nan academic institution can initiate and foster collaboration\nwith the industry.\nUniversities can also help their faculty manage the changed\ncompetitive landscape by encouraging and allowing them to\nbe more risk-taking. The comparative advantage of academic\nresearchers in AI is to do more high-risk exploration, and\nincentive structures at universities must change to account\nfor this. For example, it is unreasonable to expect a steady\nstream of papers at top-tier conferences such as NeurIPS and\nAAAI; large, well-funded industry research labs will have\nlarge advantages at writing such papers. Similarly, the grant\nfunding structure is such that it rewards safe and incremental\nresearch on popular topics; this seems to be an inherent feature\nof the way grant applications are evaluated, and it is unlikely\nto change however often funding agencies use words like\n“disruptive”. The kind of research that is favored by some of\nthe most traditional (closed-call) grant mechanisms is mostly\nthe kind of research where academic AI researchers will not be\nable to compete with industry. Therefore, universities should\nprobably avoid making grant funding a condition for hires and\npromotions. If universities are serious about incentivizing their\nfaculty to leverage their competitive advantage, they should\nreward trying and failing and promote high-risk high-gain\nfunding schemes and research initiatives. It is then likely that\nfunding agencies will follow the trend and invest even more\non basic and blue sky research.\nSuch a mindset might further open the possibilities for\nacademics to attract large amounts of funding and collec-\ntively start building their own large (foundation) models that\nwould be entirely open to any researcher. European research\nfunding, for instance, has long supported the AI-on-Demand\nPlatform2—a community-driven channel featuring open access\nAI tools—that could host such collaborative efforts on model\nbuilding and sharing. The seeds of collaborative open-source\nprojects are already planted; think of StarCoder, the recent\nlarge model built by an open-science community involving\nboth universities and industrial partners [11]. We feel it is\nonly a matter of time that more and larger academic-driven\nmodels and data will be shared openly.\nXVII. PARTING WORDS\nWe wrote this Point of View article with several purposes\nin mind. First, to share our concerns with other fellow AI\nresearchers with a hope of finding a common cause (and\na collective remedy?) as a community. Second, to offer a\nset of guidelines based on our own experiences but also the\ndiscussions we had in the academic and industrial AI venues\nwe participate or organize. Third, to spur an open dialogue\nand solicit ideas for potentially more efficient strategies for\nus all. Arguably, the list of strategies we ended up discussing\nhere are far from inclusive of all possibilities that are available\nout there; we believe, however, that they are seeds of a\nconversation that—in our opinion—is very timely.\nXVIII. ACKNOWLEDGEMENTS\nWe would like to thank all anonymous and eponymous\nreviewers for their insightful comments. This work has been\n2https://www.ai4europe.eu/\nsupported by NSF Award 1956200 and by a GoodAI award\n(JT) and from the European Union’s Horizon 2020 programme\nunder grant agreement No 951911 (GNY).\nREFERENCES\n[1] Rick Astley. Never Gonna Give You Up. RCA, New York, 1987.\n[2] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang,\nAdrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune.\nVideo pretraining (vpt): Learning to act by watching unlabeled online\nvideos. Advances in Neural Information Processing Systems, 35:24639–\n24654, 2022.\n[3] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover,\nMisha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.\nDecision transformer: Reinforcement learning via sequence modeling.\nAdvances in neural information processing systems, 34:15084–15097,\n2021.\n[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-\nsenborn,\nXiaohua\nZhai,\nThomas\nUnterthiner,\nMostafa\nDehghani,\nMatthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929, 2020.\n[5] Theodoros Galanos, Antonios Liapis, Georgios N Yannakakis, and\nReinhard Koenig. Arch-elites: Quality-diversity for urban design. In\nProceedings of the Genetic and Evolutionary Computation Conference\nCompanion, pages 313–314, 2021.\n[6] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao.\nKnowledge distillation: A survey. International Journal of Computer\nVision, 129:1789–1819, 2021.\n[7] Souhila Kaci. Working with preferences: Less is more. Springer Science\n& Business Media, 2011.\n[8] Ahmed Khalifa, Gabriella AB Barros, and Julian Togelius. Deeptingle.\nIn International Conference on Computational Creativity, 2017.\n[9] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.\nDeep learning.\nnature, 521(7553):436–444, 2015.\n[10] En Li, Liekang Zeng, Zhi Zhou, and Xu Chen. Edge ai: On-demand\naccelerating deep neural network inference via edge computing. IEEE\nTransactions on Wireless Communications, 19(1):447–457, 2019.\n[11] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis\nKocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li,\nJenny Chim, et al.\nStarcoder: may the source be with you!\narXiv\npreprint arXiv:2305.06161, 2023.\n[12] Weng Marc Lim, Asanka Gunasekara, Jessica Leigh Pallant, Jason Ian\nPallant, and Ekaterina Pechenkina.\nGenerative ai and the future of\neducation: Ragnar¨ok or reformation? a paradoxical perspective from\nmanagement educators.\nThe International Journal of Management\nEducation, 21(2):100790, 2023.\n[13] Konstantinos Makantasis, David Melhart, Antonios Liapis, and Geor-\ngios N Yannakakis. Privileged information for modeling affect in the\nwild. In 2021 9th International Conference on Affective Computing and\nIntelligent Interaction (ACII), pages 1–8. IEEE, 2021.\n[14] Keiran Paster, Sheila McIlraith, and Jimmy Ba.\nYou can’t count on\nluck: Why decision transformers fail in stochastic environments. arXiv\npreprint arXiv:2205.15967, 2022.\n[15] Tim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei\nSun, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Ida\nMomennejad, Katja Hofmann, et al. Imitating human behaviour with\ndiffusion models. arXiv preprint arXiv:2301.10677, 2023.\n[16] Christoffer Holmg˚ard Pedersen, Benedikte Mikkelsen, Julian Togelius,\nGeorgios N Yannakakis, Sebastian Risi, and Lars Henriksen. Experience\nbased game development and methods for use therewith, May 17 2022.\nUS Patent 11,331,581.\n[17] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo,\nAlexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky,\nJackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv\npreprint arXiv:2205.06175, 2022.\n[18] Sebastian Risi and Julian Togelius. Increasing generality in machine\nlearning through procedural content generation. Nature Machine Intel-\nligence, 2(8):428–436, 2020.\n[19] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev\nSatheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla,\nMichael Bernstein, et al.\nImagenet large scale visual recognition\nchallenge. International journal of computer vision, 115:211–252, 2015.\n[20] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green\nai. Communications of the ACM, 63(12):54–63, 2020.\n7\n[21] Nur Muhammad Shafiullah, Zichen Cui, Ariuntuya Arty Altanzaya, and\nLerrel Pinto. Behavior transformers: Cloning k modes with one stone.\nAdvances in neural information processing systems, 35:22955–22968,\n2022.\n[22] Richard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1), 2019.\n[23] Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja,\nFeryal Behbahani, Avishkar Bhoopchand, Nathalie Bradley-Schmieg,\nMichael Chang, Natalie Clay, Adrian Collister, et al.\nHuman-\ntimescale adaptation in an open-ended task space.\narXiv preprint\narXiv:2301.07608, 2023.\n[24] Julian Togelius, Georgios N Yannakakis, Kenneth O Stanley, and\nCameron Browne.\nSearch-based procedural content generation: A\ntaxonomy and survey. IEEE Transactions on Computational Intelligence\nand AI in Games, 3(3):172–186, 2011.\n[25] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Alma-\nhairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal\nBhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. Advances in neural information processing systems, 30,\n2017.\n[27] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot,\nand Nando Freitas. Dueling network architectures for deep reinforce-\nment learning. In International conference on machine learning, pages\n1995–2003. PMLR, 2016.\n[28] Georgios N Yannakakis and Julian Togelius. Experience-driven proce-\ndural content generation. IEEE Transactions on Affective Computing,\n2(3):147–161, 2011.\nJulian Togelius (S’05-M’07-SM’22) is an Asso-\nciate Professor in the Department of Computer Sci-\nence and Engineering, New York University, and\na co-founder of modl.ai. He works on artificial\nintelligence for games and on games for artificial\nintelligence. His current main research directions\ninvolve procedural content generation in games, gen-\neral video game playing, player modeling, and fair\nand relevant benchmarking of AI through game-\nbased competitions. Additionally, he works on topics\nin evolutionary computation, quality-diversity algo-\nrithms, and reinforcement learning. From 2018 to 2021, he was the Editor-\nin-Chief of the IEEE Transactions on Games. Togelius holds a BA from\nLund University, an MSc from the University of Sussex, and a PhD from\nthe University of Essex. He has previously worked at IDSIA in Lugano and\nat the IT University of Copenhagen.\nGeorgios N. Yannakakis (S’04-M’05-SM’14-F’24)\nis a Professor at the Institute of Digital Games,\nUniversity of Malta, and a co-founder of modl.ai.\nHe does research at the crossroads of artificial intelli-\ngence, computational creativity, affective computing,\nadvanced game technology, and human-computer\ninteraction. He has published more than 350 papers\nin the aforementioned fields and his work has been\ncited broadly. He is currently the Editor in Chief of\nIEEE TRANSACTIONS ON GAMES and an Associate\nEditor of IEEE TRANSACTIONS ON EVOLUTION-\nARY COMPUTATION.\n"
    },
    {
        "Published": "2024-03-20",
        "Title": "It's All About Your Sketch: Democratising Sketch Control in Diffusion Models",
        "Authors": "Subhadeep Koley, Ayan Kumar Bhunia, Deeptanshu Sekhri, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song",
        "Summary": "This paper unravels the potential of sketches for diffusion models,\naddressing the deceptive promise of direct sketch control in generative AI. We\nimportantly democratise the process, enabling amateur sketches to generate\nprecise images, living up to the commitment of \"what you sketch is what you\nget\". A pilot study underscores the necessity, revealing that deformities in\nexisting models stem from spatial-conditioning. To rectify this, we propose an\nabstraction-aware framework, utilising a sketch adapter, adaptive time-step\nsampling, and discriminative guidance from a pre-trained fine-grained\nsketch-based image retrieval model, working synergistically to reinforce\nfine-grained sketch-photo association. Our approach operates seamlessly during\ninference without the need for textual prompts; a simple, rough sketch akin to\nwhat you and I can create suffices! We welcome everyone to examine results\npresented in the paper and its supplementary. Contributions include\ndemocratising sketch control, introducing an abstraction-aware framework, and\nleveraging discriminative guidance, validated through extensive experiments.",
        "Page_content": "It’s All About Your Sketch: Democratising Sketch Control in Diffusion Models\nSubhadeep Koley1,2\nAyan Kumar Bhunia1\nDeeptanshu Sekhri1\nAneeshan Sain1\nPinaki Nath Chowdhury1\nTao Xiang1,2\nYi-Zhe Song1,2\n1SketchX, CVSSP, University of Surrey, United Kingdom.\n2iFlyTek-Surrey Joint Research Centre on Artificial Intelligence.\n{s.koley, a.bhunia, d.sekhri, a.sain, p.chowdhury, t.xiang, y.song}@surrey.ac.uk\nhttps://subhadeepkoley.github.io/StableSketching\nSketch\nSGDM\nControlNet\nT2I-Adapter\nOurs\nSketch\nControlNet T2I-Adapter\nOurs\nSketch\nControlNet T2I-Adapter\nOurs\nEdgemap\nControlNet\nT2I-Adapter\nOurs\nOur Results\nFigure 1. Top-left: Comparison of images generated by our method with SGDM [81], ControlNet [90], and T2I-Adapter [55]. Top-right:\nA set of photos generated by our method. Bottom: While existing methods [55, 90] generate realistic images from pixel-perfect edgemaps,\nthey perform sub-optimally for freehand abstract sketches. (Best view when zoomed in.)\nAbstract\nThis paper unravels the potential of sketches for dif-\nfusion models, addressing the deceptive promise of direct\nsketch control in generative AI. We importantly democra-\ntise the process, enabling amateur sketches to generate\nprecise images, living up to the commitment of “what\nyou sketch is what you get”.\nA pilot study underscores\nthe necessity, revealing that deformities in existing mod-\nels stem from spatial-conditioning. To rectify this, we pro-\npose an abstraction-aware framework, utilising a sketch\nadapter, adaptive time-step sampling, and discriminative\nguidance from a pre-trained fine-grained sketch-based im-\nage retrieval model, working synergistically to reinforce\nfine-grained sketch-photo association. Our approach op-\nerates seamlessly during inference without the need for tex-\ntual prompts; a simple, rough sketch akin to what you and\nI can create suffices! We welcome everyone to examine re-\nsults presented in the paper and its supplementary. Contri-\nbutions include democratising sketch control, introducing\nan abstraction-aware framework, and leveraging discrim-\ninative guidance, validated through extensive experiments.\n1. Introduction\nThis paper is dedicated to unlocking the full potential of\nyour sketches to control diffusion models [24, 25, 61]. Dif-\nfusion models [16, 24, 25, 61] have made a significant im-\npact, empowering individuals to unleash their visual cre-\nativity – consider prompts like “astronauts riding a horse\non Mars” and other “creative” ones of your own! While\nprevailing in text-to-image generation [16, 61, 64], recent\nworks [55, 81, 90] have started to question the expres-\nsive power of text as a conditioning modality. This shift\nhas led to an exploration of sketches – a modality that of-\nfers a degree of fine-grained control that is unparalleled by\ntext [13, 70], resulting in generated content of closer resem-\nblance. The promise is “what you sketch is what you get”.\nThis promise is, however, deceptive. Current works (e.g.,\nControlNet [90], T2I-Adapter [55]) predominantly focus on\ncurated edgemap-like sketches – you better sketch like a\ntrained artist, otherwise “what you get” will literally be re-\nflecting deformities captured in your (“half-decent”) sketch\n(Fig. 1). The primary goal of this paper is to democratise\nsketch control in diffusion models, empowering real am-\n1\narXiv:2403.07234v2  [cs.CV]  20 Mar 2024\nateur sketches to generate photo-precise images, ensuring\nthat “what you get” aligns with your intended sketch, re-\ngardless of how well you drew it! To achieve this, we draw\ninsights from the sketch community [37, 38, 65, 67, 87] and\nintroduce, for the first time, an awareness of sketch abstrac-\ntion (as a result of varying drawing skills) into the genera-\ntive process. This novel approach permits sketches of differ-\nent abstraction levels to guide the generation process while\nmaintaining output fidelity.\nWe conduct a pilot study to reaffirm the necessity of\nour research (Sec. 4). In which, we identify that the de-\nformed output of existing sketch-conditional diffusion mod-\nels stems from their spatial-conditioning approach – they\ndirectly translate sketch contours into the output photo\ndomain, therefore producing deformed output.\nConven-\ntional means of controlling the influence of spatial sketch-\nconditioning on the final output via weighing factors [55,\n81] or sampling tricks [90], however, require careful tuning.\nReducing output deformity by assigning less weight to the\nsketch-conditioning often makes the output more coherent\nwith the textual description, thus reducing its fidelity to the\nguiding sketch; yet, assigning higher weight to the textual\nprompt introduces lexical ambiguity [71]. On the contrary,\navoiding lexical ambiguity by assigning a higher weight to\nthe guiding sketch almost always produces deformed and\nnon-photorealistic outputs [55, 81, 90]. Last but not least,\nthe sweet spot between the conditioning weights is different\nfor different sketch instances (as seen in Fig. 2).\nAs such, our goal is to craft an effective sketch-\nconditioning strategy that not only operates without any tex-\ntual prompts during inference but is also abstraction-aware.\nAt the core of our work is a sketch adapter that transforms\nan input sketch into its equivalent textual embedding, di-\nrecting the denoising process of the diffusion model via\ncross-attention. Through the use of a smart time-step sam-\npling strategy, we ensure the adaptability of the denoising\nprocess to the abstraction level of the input sketch. Addi-\ntionally, by capitalising on the pre-trained knowledge of an\noff-the-shelf [66] fine-grained sketch-based image retrieval\n(FG-SBIR) model, we incorporate discriminative guidance\ninto our system for fine-grained sketch-photo association.\nUnlike widely used external classifier-guidance [16], our\nproposed discriminative guidance mechanism does not re-\nquire any specifically trained classifier capable of classify-\ning both noisy and real data. Lastly, even though our infer-\nence pipeline does not rely on textual prompts, we use syn-\nthetically generated textual prompts during training to learn\nthe sketch adapter with the limited sketch-photo paired data.\nOur contributions are: (i) we democratise sketch control,\nenabling real amateur sketches to generate accurate images,\nfulfilling the promise of “what you sketch is what you get”.\n(ii) we introduce an abstraction-aware framework that over-\ncomes limitations of text prompts and spatial-conditioning.\n(iii) we leverage discriminative guidance through a pre-\ntrained FG-SBIR model for fine-grained sketch-fidelity. Ex-\ntensive experiments validate the effectiveness of our method\nin addressing existing limitations in this domain.\n2. Related Works\nDiffusion Models for Vision Tasks.\nDiffusion mod-\nels [24, 25, 74] have now become the gold-standard for\ndifferent controllable image generation frameworks like\nDALL-E [57], Imagen [64], T2I-Adapter [55], Control-\nNet [90], etc. Besides image generation, several methods\nlike Dreambooth [63], Imagic [32], Prompt-to-Prompt [22],\nSDEdit [52], SKED [54] extend it for realistic image edit-\ning. Beyond image generation and editing, diffusion model\nis also used in several downstream vision tasks like recog-\nnition [43], semantic [2] and panoptic [84] segmentation,\nimage-to-image translation [79], medical imaging [15], im-\nage correspondence [78], image retrieval [39], etc.\nSketch for Visual Content Creation.\nFollowing its suc-\ncess in sketch-based image retrieval (SBIR) [3, 11, 66],\nsketches are now being used in other downstream tasks\nlike saliency detection [6], augmented reality [50, 51],\nmedical image analysis [35], object detection [14], class-\nincremental learning [4], etc.\nApart from the plethora\nof sketch-based 2D and 3D image generation and editing\nframeworks [21, 36, 47, 54, 55, 60, 81, 82, 90], sketches are\nalso getting significant traction in other visual content cre-\nation tasks like animation generation [73] and inbetween-\ning [72], garment design [12, 46], caricature generation\n[10], CAD modelling [44, 88], anime editing [28], etc.\nSketch-to-Image (S2I) Generation.\nPrior GAN-based\nS2I models typically leverage either contextual loss [49],\nmulti-stage generation [19], etc. or performs latent map-\nping [36, 60] on top of pre-trained GANs.\nAmong\ndiffusion-based frameworks, PITI [82] trains a dedicated\nencoder to map the guiding sketch to the pre-trained dif-\nfusion model’s latent manifold, SDEdit [52] sequentially\nadds noise to the guiding sketch and iteratively denoise it\nbased on a text prompt, while SGDM [81] trains an MLP\nthat maps the latent feature of the noisy images to the guid-\ning sketches in order to force the intermediate noisy images\nto closely follow the guidance sketches. Among more re-\ncent multi-conditional (e.g., depth map, colour palate, key\npose, etc.) frameworks, ControlNet [90] learns to control\na frozen diffusion model by creating a trainable copy of\nits UNet encoders and connects it with the frozen model\nwith zero-convolution [90], while T2I-Adapter [55] learns\nan encoder to extract features from the guidance signal (e.g.,\nsketch) and conditions the generation process by adding the\nguidance features with the intermediate UNet features at\neach scale. While existing methods can generate photore-\nalistic images from precise edgemaps, they struggle with\nabstract freehand sketches (see Fig. 1). Furthermore, it is\n2\na photo\nof\ncat\na photo\nof\ndolphin\na photo\nof\nrabbit\na photo\nof\nteapot\nFigure 2. Images generated by T2I-Adapter [55] for different sketch-guidance factors (ω ∈[0, 1]). Determining the optimum ω to obtain\nan ideal balance (green-bordered) between photorealism and sketch-fidelity requires manual intervention and is sample-specific. A high\nvalue of ω works well for less deformed sketches, while the same for an abstract sketch produces deformed outputs and vice-versa.\nnoteworthy that almost all of the diffusion-based S2I mod-\nels [52, 55, 81, 82, 90] rely heavily on highly-engineered\nand detailed textual prompts.\n3. Revisiting Diffusion Model (DM)\nOverview.\nDiffusion models comprises two complemen-\ntary random processes viz. “forward” and “reverse” [25]\ndiffusion. Forward diffusion process iteratively adds Gaus-\nsian noise of varying magnitude to a clean training image\nx0 ∈Rh×w×3 for t time-steps to yield a noisy image\nxt ∈Rh×w×3 as:\n  \\ mathbf  {\nx\n} _ t = \\sqrt {\\bar {\\alpha }_t}\\mathbf {x}_{0} + (\\sqrt {1-\\bar {\\alpha }_t})\\epsilon \\vspace {-0.1cm} \\label {eq:noising} \n(1)\nwhere, ϵ∼N(0, I), t∼U(0, T), and {αt}T\n1 is a pre-defined\nnoise schedule with ¯αt = Qt\ni=1 αi [25]. Reverse diffu-\nsion process trains a modified denoising UNet [62] Fθ(·),\nthat estimates the input noise ϵ ≈Fθ(xt, t) from the noisy\nimage xt at each time-step t. Fθ being trained with an l2\nloss [25] can reverse the effect of the forward diffusion pro-\ncedure. During inference, starting from a random 2D noise\nxT sampled from a Gaussian distribution, Fθ is applied it-\neratively (for T time-steps) to denoise xt at each time-step t\nto get a cleaner image xt−1, eventually leading to a cleanest\nimage x0 of the original target distribution [25].\nThe unconditional denoising diffusion process could be\nmade “conditional” by influencing the Fθ with auxiliary\nconditioning signals d (e.g., textual description [58, 61, 64],\netc.).\nThus, Fθ(xt, t, d) could perform denoising on xt\nwhile being guided by d via cross-attention [61].\nLatent Diffusion Model. Unlike standard diffusion mod-\nels [16, 25], Latent Diffusion Model [61] (a.k.a.\nStable\nDiffusion–SD) performs denoising diffusion on the latent\nspace for faster and more stable training [61]. SD first trains\nan autoencoder (consists of an encoder E(·) and a decoder\nD(·) in series) to convert the input image x0 ∈Rh×w×3 to\nits latent representation z0 = E(x0) ∈R\nh\n8 × w\n8 ×d. Later,\nSD trains a modified denoising UNet [62] ϵθ(·) to perform\ndenoising directly on the latent space. The textual prompt d\nupon passing through a CLIP textual encoder [56] T(·) pro-\nduces the corresponding token-sequence that influences the\nintermediate feature maps of the UNet via cross-attention\n[61]. SD trains with an l2 loss as:\n  \\ m athcal {L}_{\\ t ext {S D} } = \\mat\nhbb {E}_{\\mathbf {z}_t,t,{d},\\epsilon } ({||\\epsilon -\\epsilon _{\\theta }(\\mathbf {z}_t,t,\\mathbf {T}(d))||}_2^2) \\label {eq:sd} \n(2)\nDuring inference, SD discards E(·), directly sampling a\nnoisy latent zT from a Gaussian distribution [61]. It then\nestimates noise from zT iteratively for T iterations via ϵθ\n(conditioned on d) to obtain a clean latent ˆz0. The frozen\ndecoder generates the final image as: ˆx0 = D(ˆz0) [61].\n4. What’s wrong with Sketch-to-Image DM\nRecent controllable image generation methods like Control-\nNet [90], T2I-Adapter [55], etc. offer extreme photorealism,\nsupporting different conditioning signals (e.g., depth map,\nlabel mask, edgemap, etc.) However, conditioning the same\nfrom sparse freehand sketches is often sub-optimal (Fig. 1).\nSketch vs. Other Conditional Inputs. Sparse and binary\nfreehand sketches while good for providing fine-grained\nspatial cues [6, 14, 89], often depict significant shape-\ndeformity [17, 23, 65] and hold far less contextual infor-\nmation [79] than other pixel-perfect conditioning signals\nlike depth maps, normal maps, or pixel-level segmentation\nmasks. Hence, conditioning from freehand sketches is non-\ntrivial and needs to be handled uniquely unlike the rest of\nthe pixel-perfect conditioning signals.\nSketch vs. Text Conditioning: A Trade-off. Previous S2I\ndiffusion models [55, 81, 90] exhibit two major challenges.\nFirstly, quality of generated outputs being highly depen-\ndent on precise and accurate textual prompts [90], incon-\nsistencies or lack of suitable prompts can negatively impact\n(Fig. 3) the results [55, 90]. Secondly, ensuring a balance\nbetween the influence of sketch and text-conditioning on\nthe final output requires manual intervention, which can be\nchallenging. Adjusting the weighting of these factors of-\nten results in a trade-off between output’s coherence with\nthe text and fidelity to the sketch [55]. In some cases, giv-\ning higher weight to text can lead to lexical ambiguity [71],\nwhile prioritising sketch tends to produce distorted and non-\nphotorealistic results [55, 81]. Achieving photorealistic out-\nput from existing S2I DMs [55, 81] thus demands meticu-\n3\nlous fine-tuning of these weights, where the optimal balance\nvaries for different sketch instances as seen in Fig. 2.\nSketch\nSGDM\nControlNet\nT2I-Adapter\nOurs\nFigure 3. Passing null prompt (i.e., “ ”) in existing [55, 81, 90]\nsketch-conditioned DMs significantly distorts the output quality.\nProblems with Spatial-Conditioning for Sketches. We\nidentify that the deformed and non-photorealistic (e.g.,\nedge-bleeding in Fig. 2) outputs of existing sketch-\nconditional DMs [55, 81, 90] are primarily a consequence\nof their spatial-conditioning approach. T2I-Adapter [55]\ndirectly integrates the spatial features of the conditioning-\nsketch into the UNet encoder’s feature maps, while Control-\nNet [90] applies this to skip connections and middle blocks.\nSGDM [81], on the other hand, projects the latent features\nof noisy images to spatial edgemaps guiding the denois-\ning process towards following the edgemaps.\nAddition-\nally, these models are trained and tested with synthetically-\ngenerated [7, 76, 83] edgemaps/contours rather than real\nfreehand sketches. Instead, we aim to devise an effective\nconditioning strategy for real freehand sketches while en-\nsuring that the output faithfully captures an end-users’ se-\nmantic intent [36] without any deformities.\n5. Proposed Methodology\nOverview. We aim to eliminate spatial sketch-conditioning\nby converting the input sketch into an equivalent fine-\ngrained textual embedding,\nthereby preserving users’\nsemantic-intent without pixel-level spatial alignment. Con-\nsequently, our method would alleviate issues pertaining to\nspatial distortions (e.g., deformed shapes, edge-bleeding,\netc.)\nwhile maintaining fine-grained fidelity to the input\nsketch. We introduce three salient designs (Fig. 4) – (i) fine-\ngrained discriminative loss for maintaining the fine-grained\nsketch-photo correspondence (Sec. 5.2). (ii) guiding our\ntraining process with textual prompts (not used during infer-\nence), as a means of super-concept preservation (Sec. 5.3).\nFinally, (iii) unlike the uniform time-step (t) sampling of\nprior arts [81, 90], we introduce a sketch-abstraction-aware\nt-sampling (Sec. 5.4). For a highly abstract sketch, a higher\nprobability is assigned to larger t and vice-versa.\n5.1. Sketch Adapter\nAiming to mitigate the evident disadvantages (Sec. 4) of\ndirect spatial-conditioning approach of existing sketch-\nconditional diffusion models (e.g., ControlNet [90], T2I-\nAdapter [55], etc.), we take a parallel approach to “sketch-\ncondition” the generation process via cross-attention. In\nthat, instead of treating the input sketches spatially, we\nencode them as a sequence of feature vectors [42] as an\nForward\nDiffusion\nForward\nDiffusion\nSketch Adapter\nTweedie's\nformula (Eq. 4)\n\"a black and white cat\nsitting beside a brick wall\"\nFigure 4. Our overall training pipeline. (More in the text.)\nequivalent fine-grained textual embedding. Direct spatial-\nconditioning enforces the model to remember the contex-\ntual information rather than understanding it [85].\nThis\nresults in a direct translation of the strong sketch features\n(e.g., stroke boundaries) into the output photo. To over-\ncome this, we aim to increase the hardness of the problem\nby compressing the spatial sketch input to a bottlenecked-\nrepresentation via sketch adapter.\nIn particular, given a sketch s, we use a pre-trained\nCLIP [56] ViT-L/14 image encoder V(·) to generate its\npatch-wise sketch embedding s = V(s) ∈R257×1024. Our\nsketch adapter A(·) consists of 1-dimensional convolutional\nand vanilla attention [80] modules followed by FC layers.\nThe convolutional and FC layers handle the dimension mis-\nmatch between text and sketch-embedding (i.e., R257×1024\n→R77×768), whereas the attention module tackles the large\nsketch-text domain gap. The patch-wise sketch embedding\ns upon passing through A(·) generates the equivalent tex-\ntual embedding as ˆs = A(s) ∈R77×768. Now replacing\nthe textual conditioning in Eq. (2) with our sketch adapter\nconditioning, the modified loss objective becomes:\n  \\ m athcal {L}_{\\ t ext {S D} } = \\mathbb\n {E}_{\\mathbf {z}_t,t,{s},\\epsilon } ({||\\epsilon -\\epsilon _{\\theta }(\\mathbf {z}_t,t,\\mathcal {A}(\\mathbf {V}(s)))||}_2^2) \\vspace {-0.1cm} \\label {eq:sd_modified} \n(3)\nOnce trained, the sketch adapter efficiently converts an\ninput sketch s into its equivalent textual embedding ˆs, which\nthrough cross-attention controls the denoising process of\nSD [61]. Nonetheless, conditioning solely via the proposed\nsketch adapter poses multiple challenges – (i) sparse free-\nhand sketches and pixel-perfect photos depict a huge do-\nmain gap. The standard l2 loss [61] of a text-to-image dif-\nfusion model is not enough to ensure a fine-grained match-\ning between sketch and photo. (ii) training a robust sketch\nadapter from the limited available sketch-photo pairs is dif-\nficult. Consequently, during training, we aim to use pseudo\ntexts as a learning signal to guide the training of our sketch\nadapter. Please note, our inference pipeline does not in-\nvolve any textual prompts. (iii) the sketch adapter treats\nall sketch samples equally regardless of their abstraction\nlevels. While this equal treatment might suffice for dense\npixel-level conditioning, it might be inadequate for sparse\nsketches, as different sketches depicting different abstrac-\ntion levels are not semantically-equal [5, 86].\n4\n5.2. Fine-Grained Discriminative Learning\nTo ensure a fine-grained matching between sparse freehand\nsketches and pixel-perfect photos, we utilise a pre-trained\nfine-grained (FG) SBIR model [66] Fg(·).\nA photo sits\nclose to its paired sketch in a pre-trained FG-SBIR model’s\ndiscriminative latent embedding space compared to other\nunpaired ones [66]. Previous attempts at guiding the dif-\nfusion process with external discriminative models include\nclassifier-guidance [16] that require a pre-trained fixed-class\nclassifier capable of classifying both noisy and real data [16]\nto guide the denoising procedure [16]. However, as our\nfrozen FG-SBIR model is not trained on noisy data, it re-\nquires a clean image at each t, to perform in an off-the-shelf\nmanner. Now, for each t, as the denoiser estimates that noise\nϵt ≈ϵθ(zt, t, A(V(s))), which was added to z0 to get zt\nduring forward diffusion, we can use Eq. (1) to recreate z0\nfrom ϵt. Specifically, we utilise Tweedie’s formula [34] to\nestimate [1, 40, 85] the clean latent image ˆz0 from the tth-\nstep noisy latent zt in a single-step for efficient training as:\n  \\Hat {\\ ma t hb f  {z }}_0(\\ ma thbf {z}\n_t) := \\frac {\\mathbf {z}_t-\\sqrt {1-\\bar {\\alpha }_t}~\\epsilon _\\theta (\\mathbf {z}_t, t, \\mathcal {A}(\\mathbf {V}(s)))}{\\sqrt {\\bar {\\alpha }_t}} \\vspace {-0.2cm} \n(4)\nˆz0 upon passing through SD’s [61] frozen VAE decoder\nD(·) approximates the clean image ˆx0 (Sec. 3). To learn\nthe sketch adapter A, we use a discriminative SBIR loss\nthat calculates cosine similarity δ(·, ·) between s and ˆx0 as:\n  \\la b e l  {eq:fg s bir_loss} \\mathcal {L}_{\\text {SBIR}} = 1 - \\delta \\left ({\\mathcal {F}_g(s) \\cdot \\mathcal {F}_g(\\Hat {\\mathbf {x}}_0)}\\right ) \\vspace {-0.1cm} \n(5)\n5.3. Super-concept Preservation Loss\nAn inherent complementarity exists between sketch and text\n[13]. A textual caption of an image can correspond to mul-\ntiple plausible photos in the embedding space. Adding a\nsketch with it however, narrows down the scope to a par-\nticular image [13, 70] (i.e., fine-grained). We posit that\na textual description being less fine-grained than a sketch\n[13, 75, 85], acts as a super-concept of the corresponding\nsketch. Although we do not use any textual prompt during\ninference, we aim to use them during training of our sketch\nadapter. Text-to-image diffusion models being trained on a\nlarge corpus of text-image pairs [61], implicitly hold supe-\nrior text-to-image generation capability (although not fine-\ngrained [18]). We thus aim to use this super-concept knowl-\nedge from textual descriptions to distil the large-scale text-\nto-image knowledge of a pre-trained SD to train our sketch-\nadapter with limited sketch-photo paired data.\nAs our sketch-photo (s, p) dataset [69] lacks paired tex-\ntual captions, we use a pre-trained state-of-the-art image\ncaptioner [45] to synthetically generate caption d for every\nground truth photo p. Then, at each t, the noise predicted\nthrough text-conditioning (T(d)) acts as a reference to cal-\nculate a regularisation loss to learn the sketch adapter A as:\n  \\m a thcal {L }_ {\\tex t  {reg} } = ||\\epsilo\nn _\\theta (\\mathbf {z}_t,t,\\mathbf {T}(d))-\\epsilon _\\theta (\\mathbf {z}_t,t,\\mathcal {A}(\\mathbf {V}(s)))||_2^2 \\vspace {-0.1cm} \n(6)\n5.4. Abstraction-aware Importance Sampling\nExisting literature [26, 27, 55, 85] indicates that during the\ndenoising process, high-level semantic structures of the out-\nput image tend to manifest in the early stages, while finer\nappearance details emerge later.\nSynthetic pixel-perfect\nconditioning signals (e.g., depth map [59], key pose [8],\nedgemap [7], etc.)\nexhibit minimal subjective abstrac-\ntion [23]. In contrast, human-drawn freehand sketches ex-\nhibit varying abstraction levels, influenced by factors like\nskill, style, and subjective interpretation [65, 67]. Thus,\nuniform time-step sampling [27] for abstract sketches may\ncompromise output generation quality and sketch-fidelity.\nHence, we propose adjusting the time-step sampling proce-\ndure based on the input sketch’s abstraction level [87]. For\nhighly abstract sketches, we skew the sampling distribution\nto emphasise the later t values that govern the high-level\nsemantics in the output. Instead of sampling the time-step\nfrom uniform distribution t∼U(0, T), we sample from:\n  \\ma t h\nc\na\nl  { S}_\\o\nm\nega (t)=\\frac {1}{T}\\left (1-\\omega ~\\mathrm {cos}\\frac {\\pi t}{T}\\right ) \\vspace {-0.1cm} \\label {eq:samplig} \n(7)\nwhere, Sω(·) is our abstraction-aware t-sampling func-\ntion, where increasing or decreasing ω ∈(0, 1], controls\nthe skewness of this sampling probability density function.\nPushing ω towards 1 increases the probability of sampling\na larger t value (Fig. 5). We aim to make this skewness-\ncontrolling ω value sketch-abstraction specific.\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\nTime-steps (t) !\n0\n0.5\n1\n1.5\n2\nS!(t)\n#10!3\n! = 0\n! = 0:5\n! = 1\nFigure 5. Abstraction-aware t-sampling function for different ω.\nNow the question remains as to how we can quantify\nthe abstraction level of a freehand sketch. Taking inspi-\nration from [87], we design a CLIP [56]-based (a generic\nclassifier) sketch classifier with a MagFace [53]-based loss\nwhere the l2-norm of a sketch feature a ∈[0, 1], denotes\nhow closely it sits from its respective class-centre. While\na →1 represents edgemap-like less abstract sketches, a\n→0 denotes highly-abstract and deformed ones. We posit\nthat edgemaps being less deformed (i.e., easier to clas-\nsify), will implicitly stay close to their respective class cen-\ntres in the latent space. Whereas, freehand sketches being\nhighly abstract and deformed (i.e., harder to classify), will\nbe placed away from their corresponding class centres. We\nthus train the sketch classifier with sketches and synthesised\n[9] edgemaps of the associated photos from Sketchy [69],\nusing our classification loss:\n  \\f o otno\nte size \\m athcal \n{L }_{\\tex t {abs} }  \n= -\\m at hrm  { l og}\\frac {e^{s~\\mathrm {cos}(\\theta _{y_i}+m(\\mathbf {s}_i))}}{e^{s~\\mathrm {cos}(\\theta _{y_i}+m(\\mathbf {s}_i))}+\\sum _{j\\neq y_i} e^{s~\\mathrm {cos}~\\theta _j}}+\\lambda _gg(\\mathbf {s}_i) \n(8)\n5\nwhere s is a global scalar value, θyi is the cosine similar-\nity between extracted global visual feature (from CLIP [56]\nvisual encoder) of the ith sketch sample si = V(si) ∈Rd\nwith l2-normalisation, and jth class centre wj ∈Rd com-\nputed from ground truth class labels by CLIP [56] text en-\ncoder.\nm(si) is the magnitude-aware margin parameter\nm(si) =\n(um−lm)\n(ua−la)la+lm , where lm, um denotes the lower\nand upper bounds of the margin, and la, ua denotes that\nof the feature magnitude. g(si) is a hyper-parameter (λg)-\ncontrolled regularisation term (see [53] for more details).\nWith the trained classifier, given a sketch s, the scalar ab-\nstraction score a ∈[0, 1] is given by the l2-norm of the ex-\ntracted sketch feature V(s). To keep parity with ω, we com-\nplement a to get the sketch instance-specific ω ←(1 −a),\nfollowed by empirically clipping ω in the range [0.2, 0.8].\nIn summary, we train the sketch adapter A(·) using\nsketch-abstraction-aware t-sampling with a total loss of\nLtotal=λ1LSD+λ2LSBIR+λ3Lreg. During inference, we com-\npute the abstraction score of the input sketch, taking l2-\nnorm of classifier feature. Based on the abstraction level,\nwe perform t-sampling. The input sketch passing through\nA controls the diffusion procedure and generates the output.\n6. Experiments\nDataset and Implementation Details.\nWe train and\nevaluate our model on the Sketchy dataset [69] containing\n12, 500 images from 125 categories with at least 5 sketches\nper image with fine-grained association. For training and\nevaluation, we split this dataset in 90:10. We use Stable\nDiffusion v1.5 [61] in all experiments with a CLIP [56] em-\nbedding dimension d = 768. The sketch adapter is trained\nwith a learning rate of 10−4, keeping the SD model, FG-\nSBIR backbone, and CLIP encoders frozen. We train our\nmodel for 50 epochs using AdamW [48] optimiser with 0.09\nweight decay, and batch size of 8. Values of λ1,2,3 are set to\n1, 0.5, and 0.1, empirically.\nEvaluation Metrics.\nFollowing [36, 55, 90], we quanti-\ntatively evaluate the generation quality and sketch-fidelity\nwith four metrics – Frech`et Inception Distance-InceptionV3\n(FID-I) [31] and CLIP (FID-C) [41] calculates the simi-\nlarity between generated and real images using pre-trained\nInceptionV3 [77] and CLIP [56] ViT-B/32 models respec-\ntively. Lower values of FID-I and FID-C depict better gen-\neration quality. We measure the output image’s fidelity to\nthe input sketch using Fine-Grained Metric (FGM) [36]\nwhich computes the cosine similarity between them via a\npre-trained FG-SBIR model [66], where a higher value de-\nnotes better fine-grained correspondence. Additionally, we\nalso perform a human study to collect Mean Opinion Score\n(MOS) [29]. Here, we asked 25 non-artist users to draw 40\nsketches each, and rate the generated photos on a discrete\nscale (interval=0.5) of [1, 5] (worst to best) based on output\nphotorealism and sketch-fidelity. For each method, we com-\npute the final MOS by averaging all its 1000 MOS values.\nCompetitors. We compare against different diffusion and\nGAN-based state-of-the-art (SOTA) S2I models and two\nbaselines. (i) Sketch-only Baselines: To alleviate the neces-\nsity of text, B-Classification first trains a prompt learning-\nbased sketch classifier [33] that classifies every sketch into\none of the predefined classes. From predicted class labels,\nit forms a textual prompt (i.e., “a photo of [CLASS]”) to\ngenerate images using a frozen text-to-image SD model\n[61]. Given the input sketches, B-Captioning first gener-\nates detailed captions using a pre-trained image captioner\n[45] from their paired photos, which are then used to gen-\nerate images from a frozen SD model [61]. (ii) SOTAs:\nAmong diffusion-based SOTAs, we compare with Control-\nNet [90], T2I-Adapter [55], SGDM [81], and PITI [82].\nWe also compare qualitatively against two GAN-based S2I\nparadigms viz. Pix2Pix [30] and CycleGAN [91]. While\nwe train ControlNet [90], T2I-Adapter [55], and PITI [82]\non the entire Sketchy [69] train set, we train pix2pix [30],\nand CycleGAN [91] individually for each of the depicted\nclasses (Fig. 6) from scratch with Sketchy [69] sketch-\nphoto pairs.\nWe only perform a qualitative comparison\nwith SGDM [81] by taking the results directly from the pa-\nper, as their model weights/code are unavailable. Notably,\nfor diffusion-based SOTAs [55, 82, 90], we use an addi-\ntional fixed textual prompt “a photo of [CLASS]”, replac-\ning [CLASS] with class-labels of respective input sketches.\n6.1. Performance Analysis & Discussion\nResult Analysis.\nAmong GAN-based methods, pix2pix\n[30] and CycleGAN [91] depict visible deformities (Fig. 6)\nmostly due to their weaker [16] GAN-based generator,\ncompared to an internet-scale pre-trained SD model [61].\nAmong diffusion-based SOTAs, although SGDM [81] gen-\nerates plausible colour schemes and styles, outputs ex-\nhibit substantial deformations (Fig. 1). A similar observa-\ntion can be made for PITI [82], where generated images\nlook non-photorealistic with pronounced edge-adherence\n(Fig. 6). Whereas, edge-bleeding (Fig. 6) is quite frequent\nfor T2I-Adapter [55]. ControlNet [90] surpasses PITI [82],\nSGDM [81], and T2I-Adapter [55] in terms of photoreal-\nism but mostly follows the input sketch boundaries (Fig. 6).\nContrarily, images generated by our method are more pho-\ntorealistic with fewer deformities, capturing semantic-intent\nwithout transmitting edge boundaries in the output. Quan-\ntitative results presented in Tab. 1 show B-Caption to sur-\npass B-Classification (by 0.11 FGM) thanks to the com-\nparatively higher [45] generalisation potential of the cap-\ntioning model [45] than the generic sketch classifier [33].\nNonetheless, our method exceeds these baselines both in\nterms of generation quality and sketch-fidelity with an FID-\nC of 16.20 and FGM of 0.81. Due to its superior condi-\ntioning strategy, ControlNet [90] achieves the lowest FID-I\n6\nSketch\nOurs\npix2pix\nCycleGAN\nControlNet\nT2I-Adapter\nPITI\nFigure 6. Qualitative comparison with SOTA sketch-to-image generation models on Sketchy [69]. For ControlNet [90], T2I-Adapter [55],\nand PITI [82], we use the fixed prompt “a photo of [CLASS]”, with [CLASS] replaced with corresponding class-labels of the input sketches.\namong all prior SOTAs (Tab. 1). Although less pronounced\nin terms of FID-I/FID-C, our method offers the highest fine-\ngrained sketch-fidelity with 23.45% FGM improvement of\nover ControlNet [90]. Finally, thanks to the photorealistic\ngeneration quality and fine-grained sketch correspondence,\nour method surpasses competitors in terms of MOS value\nfrom user-study with an average 1.36 ± 0.2 point improve-\nment. Notably, unlike ours, image generation via diffusion-\nbased competitors needs textual prompts, the absence of\nwhich results in much worse output quality (Fig. 3).\nTable 1. Benchmarks on the Sketchy [69] dataset.\nMethods\nFID-I ↓\nFID-C ↓\nFGM ↑\nMOS ↑\nµ ± σ\nControlNet [90]\n26.68\n21.22\n0.62\n3.68±0.2\nT2I-Adapter [55]\n26.94\n18.92\n0.56\n3.11±0.6\nPITI [82]\n84.71\n25.85\n0.23\n2.64±0.3\nB-Classification\n28.93\n19.01\n0.36\n3.13±0.2\nB-Captioning\n28.31\n18.81\n0.47\n3.21±0.4\nProposed\n25.07\n16.20\n0.81\n4.52±0.1\nGeneralisation Potential. As our method alleviates the di-\nrect spatial influence of input sketches in the denoising pro-\ncess, it enables generalisation across multiple dimensions.\nFig. 7 shows that our sketch-adapter trained on Sketchy,\ngeneralises well on random sketch samples from TU-Berlin\n[17] and QuickDraw [20] datasets, on synthetically gener-\nated [7] edgemaps, and to different stroke-styles. Further-\nmore, as our sketch adapter does not distort the original\ntext-to-image pre-training of the frozen SD model, the same\nadapter could be used to perform sketch-conditional gener-\nation from other versions of the SD model (Fig. 8).\nFigure 7. Examples showing generalisation potential across dif-\nferent datasets (left) and stroke-styles (right).\nSketch\nSD v1.5\nSD v1.4\nSketch\nSD v1.5\nSD v1.4\nFigure 8. Illustration of cross-model generalisation. Our method\ntrained with SD v1.5 [61], performs well on other unseen SD vari-\nants (e.g., v1.4) without further fine-tuning.\nRobustness and Sensitivity. Amateur freehand sketching\noften introduces irrelevant and noisy strokes [5]. We thus\ndemonstrate our model’s resilience to such strokes by pro-\ngressively adding them during inference, and assessing its\nperformance. On the other hand, to judge our model’s sta-\nbility against partially-complete sketches, we render input\nsketches at {25, 50, 75, 100}% prior to generation. As our\nmethod is devoid of direct spatial-conditioning, outputs re-\nmain relatively stable (Fig. 9) even for spatially distorted\nsketches (e.g., noisy or partially-complete).\nFine-grained Semantic Editing.\nHarnessing the large-\nscale pre-training of the frozen SD model [61], our method\n7\nFigure 9. Examples depicting the effect of adding noisy strokes\n(left) and generation from partially-completed sketches (right).\nSketch\nOurs\nEdits\nOurs\nSketch\nOurs\nEdits\nOurs\nFigure 10. Our method seamlessly transfers local semantic edits\non input sketches into output photos. (Best view when zoomed in.)\nenables fine-grained semantic editing. Here, fixing the gen-\neration seed, and performing local semantic edits in the\nsketch-domain produces seamless edited images (Fig. 10).\n6.2. Ablation on Design\n[i] Importance of Sketch Adapter.\nOur sketch adapter\n(Sec. 5.1) converts an input sketch to its corresponding tex-\ntual equivalent embedding. To judge its efficacy, we replace\nit with simple convolutional and FC-layers converting the\nR257×1024 sketch embedding to equivalent R77×768 textual\nembedding. Although less pronounced in FID scores, the\nFGM score plummets substantially (49.38%) in case of w/o\nSketch adapter (Tab. 2), indicating the significance of the\nproposed adapter in maintaining high sketch-fidelity.\n[ii] Why Discriminative Learning? Fine-grained discrim-\ninative loss (Eq. 5) helps the conditioning process by dis-\ntilling knowledge learned inside a pre-trained FG-SBIR\nmodel. As seen in Tab. 2, a noticeable FGM drop (44.44%)\nfor w/o Discriminative learning indicates that fine-grained\nsketch-conditioning is incomplete without explicit discrim-\ninative learning via LSBIR.\n[iii] Does Abstraction-aware Importance Sampling\nhelp?\nUnlike existing sketch-conditional DMs, we take\nfreehand sketch abstraction into account via abstraction-\naware t-sampling. Omitting it results (Tab. 2) in a sharp\nincrease in FID-I scores (26.64%). We hypothesise that in\nabsence of the proposed adaptive t-sampling, the system\ntreats all sketches equally, regardless of their abstraction\nlevel, resulting in sub-optimal performance.\n[iv] Impact of Super-concept Preservation.\nAlthough\nour inference procedure does not use any textual prompt,\nwe employ them during our training process to facilitate\nthe preservation of super-concepts. Eliminating this again\ndestabilises the system causing an additional 15.06% and\n17.28% decline in FID-C and FGM scores (Tab. 2). This\njustifies our incorporation of synthetic text prompts during\ntraining, as it aligns well with the original text-to-image\ngeneration objective of the pre-trained SD model [61]. Vi-\nsual ablation results are presented in Fig. 11.\nSketch\nOnly sketch\nadapter\n+ Abs.-aware \nt-sampling\n+ Super-concept\npreservation\n+ FG disc.\nlearning\nFigure 11. Visual ablation of different design components.\nTable 2. Ablation on design.\nMethods\nFID-I ↓\nFID-C ↓\nFGM ↑\nw/o Sketch adapter\n29.23\n20.34\n0.41\nw/o Discriminative learning\n29.14\n19.97\n0.45\nw/o Super-concept preservation\n27.21\n18.64\n0.67\nw/o Abs.-aware t-sampling\n31.75\n23.17\n0.55\nOurs (SD v1.4)\n26.12\n17.09\n0.77\nOurs-full\n25.07\n16.20\n0.81\n6.3. Failure Cases & Future Works\nDespite showcasing superior generation quality without sig-\nnificant deformations, our method has a few limitations. For\nInstance, it sometimes struggles to determine the correct\nclass of the input due to categorical-ambiguity, especially\nwhen two different objects look very similar shape-wise\n(Fig. 12) in their abstract and deformed sketch forms (e.g.,\napple vs. pear, guitar vs. violin). In future, we aim to extend\nour method with the flexibility to include additional class\nlabels. The sketch+label composed-conditioning [68] might\nmitigate the categorical-ambiguity of confusing classes.\nApple Sketches\nPear Sketches\nZebra Sketches\nHorse Sketches\nFigure 12. Failure cases where sketches from certain classes (e.g.,\nzebra) produce images from other similar-looking classes (e.g.,\nhorse) or vice-versa. Please note that we do not use text prompts.\n7. Conclusion\nOur work takes a significant step towards democratising\nsketch control in diffusion models. We exposed the lim-\nitations of current approaches, showcasing the deceptive\npromise of sketch-based generative AI. By introducing an\nabstraction-aware framework, featuring a sketch adapter,\nadaptive time-step sampling, and discriminative guidance,\nwe empower amateur sketches to yield precise, high-fidelity\nimages without the need for textual prompts during infer-\nence. We welcome the community to scrutinise our results.\nPlease refer to the demo video for a detailed real-time com-\nparison with state-of-the-arts.\n8\nReferences\n[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\nDiffusion for Text-driven Editing of Natural Images.\nIn\nCVPR, 2022. 5\n[2] Dmitry\nBaranchuk,\nIvan\nRubachev,\nAndrey\nVoynov,\nValentin Khrulkov, and Artem Babenko. Label-Efficient Se-\nmantic Segmentation with Diffusion Models. In ICLR, 2021.\n2\n[3] Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Aneeshan\nSain, Yongxin Yang, Tao Xiang, and Yi-Zhe Song. More\nPhotos are All You Need: Semi-Supervised Learning for\nFine-Grained Sketch Based Image Retrieval. In CVPR, 2021.\n2\n[4] Ayan Kumar Bhunia, Viswanatha Reddy Gajjala, Subhadeep\nKoley, Rohit Kundu, Aneeshan Sain, Tao Xiang, and Yi-Zhe\nSong. Doodle It Yourself: Class Incremental Learning by\nDrawing a Few Sketches. In CVPR, 2022. 2\n[5] Ayan Kumar Bhunia, Subhadeep Koley, Abdullah Faiz\nUr Rahman Khilji, Aneeshan Sain, Pinaki Nath Chowdhury,\nTao Xiang, and Yi-Zhe Song. Sketching Without Worrying:\nNoise-Tolerant Sketch-Based Image Retrieval.\nIn CVPR,\n2022. 4, 7\n[6] Ayan Kumar Bhunia, Subhadeep Koley, Amandeep Kumar,\nAneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, and Yi-\nZhe Song. Sketch2Saliency: Learning to Detect Salient Ob-\njects from Human Drawings. In CVPR, 2023. 2, 3\n[7] John Canny. A Computational Approach to Edge Detection.\nIEEE TPAMI, 1986. 4, 5, 7\n[8] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A.\nSheikh. OpenPose: Realtime Multi-Person 2D Pose Estima-\ntion using Part Affinity Fields. IEEE TPAMI, 2019. 5\n[9] Caroline Chan, Fredo Durand, and Phillip Isola. Informative\nDrawings: Learning to generate line drawings that convey\ngeometry and semantics. In CVPR, 2022. 5\n[10] Dar-Yen\nChen,\nSubhadeep\nKoley,\nAneeshan\nSain,\nPinaki Nath Chowdhury, Tao Xiang, Ayan Kumar Bhunia,\nand Yi-Zhe Song.\nDemoCaricature: Democratising Cari-\ncature Generation with a Rough Sketch.\nIn CVPR, 2024.\n2\n[11] Pinaki\nNath\nChowdhury,\nAyan\nKumar\nBhunia,\nViswanatha Reddy Gajjala, Aneeshan Sain, Tao Xiang,\nand Yi-Zhe Song. Partially Does It: Towards Scene-Level\nFG-SBIR With Partial Input. In CVPR, 2022. 2\n[12] Pinaki Nath Chowdhury, Tuanfeng Wang, Duygu Ceylan,\nYi-Zhe Song, and Yulia Gryaditskaya. Garment ideation: It-\nerative view-aware sketch-based garment modeling. In 3DV,\n2022. 2\n[13] Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan\nSain, Subhadeep Koley, Tao Xiang, and Yi-Zhe Song.\nSceneTrilogy: On Human Scene-Sketch and its Complemen-\ntarity with Photo and Text. In CVPR, 2023. 1, 5\n[14] Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan\nSain, Subhadeep Koley, Tao Xiang, and Yi-Zhe Song. What\nCan Human Sketches Do for Object Detection? In CVPR,\n2023. 2, 3\n[15] Bram de Wilde, Anindo Saha, Richard PG ten Broek, and\nHenkjan Huisman.\nMedical diffusion on a budget: tex-\ntual inversion for medical image generation. arXiv preprint\narXiv:2303.13430, 2023. 2\n[16] Prafulla Dhariwal and Alexander Nichol. Diffusion Models\nBeat GANs on Image Synthesis. In NeurIPS, 2021. 1, 2, 3,\n5, 6\n[17] Mathias Eitz, James Hays, and Marc Alexa. How do humans\nsketch objects? ACM TOG, 2012. 3, 7\n[18] Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin\nHuang.\nExpressive Text-to-Image Generation with Rich\nText. In ICCV, 2023. 5\n[19] Arnab Ghosh, Richard Zhang, Puneet K Dokania, Oliver\nWang, Alexei A Efros, Philip HS Torr, and Eli Shechtman.\nInteractive Sketch & Fill: Multiclass Sketch-to-Image Trans-\nlation. In CVPR, 2019. 2\n[20] David Ha and Douglas Eck.\nA Neural Representation of\nSketch Drawings. In ICLR, 2017. 7\n[21] Cusuh Ham, Gemma Canet Tarres, Tu Bui, James Hays, Zhe\nLin, and John Collomosse. Cogs: Controllable generation\nand search from sketch and style. In ECCV, 2022. 2\n[22] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-Prompt Image\nEditing with Cross Attention Control. In ICLR, 2022. 2\n[23] Aaron Hertzmann. Why Do Line Drawings Work? A Real-\nism Hypothesis. Perception, 2020. 3, 5\n[24] Jonathan Ho and Tim Salimans. Classifier-Free Diffusion\nGuidance. arXiv preprint arXiv:2207.12598, 2022. 1, 2\n[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Dif-\nfusion Probabilistic Models. In NeurIPS, 2020. 1, 2, 3\n[26] Ziqi Huang, Kelvin CK Chan, Yuming Jiang, and Ziwei Liu.\nCollaborative Diffusion for Multi-Modal Face Generation\nand Editing. In CVPR, 2023. 5\n[27] Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin CK Chan,\nand Ziwei Liu. ReVersion: Diffusion-Based Relation Inver-\nsion from Images. arXiv preprint arXiv:2303.13495, 2023.\n5\n[28] Zhengyu Huang, Haoran Xie, Tsukasa Fukusato, and\nKazunori Miyata.\nAniFaceDrawing:\nAnime Portrait\nExploration during Your Sketching.\narXiv preprint\narXiv:2306.07476, 2023. 2\n[29] Quan Huynh-Thu, Marie-Neige Garcia, Filippo Speranza,\nPhilip Corriveau, and Alexander Raake.\nStudy of Rating\nScales for Subjective Quality Assessment of High-Definition\nVideo. IEEE TBC, 2010. 6\n[30] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\nEfros. Image-to-Image Translation with Conditional Adver-\nsarial Networks. In CVPR, 2017. 6, 12\n[31] Tero Karras, Samuli Laine, and Timo Aila.\nA Style-\nBased Generator Architecture for Generative Adversarial\nNetworks. In CVPR, 2019. 6\n[32] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-Based Real Image Editing with Diffusion Models. In\nCVPR, 2023. 2\n[33] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad\nMaaz, Salman Khan, and Fahad Shahbaz Khan.\nMaPLe:\nMulti-modal Prompt Learning. In CVPR, 2023. 6\n9\n[34] Kwanyoung Kim and Jong Chul Ye. Noise2Score: Tweedie’s\nApproach to Self-Supervised Image Denoising without\nClean Images. In NeurIPS, 2021. 5\n[35] Kazuma Kobayashi, Lin Gu, Ryuichiro Hataya, Takaaki\nMizuno, Mototaka Miyake, Hirokazu Watanabe, Masamichi\nTakahashi,\nYasuyuki\nTakamizawa,\nYukihiro\nYoshida,\nSatoshi Nakamura, et al. Sketch-based Medical Image Re-\ntrieval. arXiv preprint arXiv:2303.03633, 2023. 2\n[36] Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain,\nPinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song. Pic-\nture that Sketch: Photorealistic Image Generation from Ab-\nstract Sketches. In CVPR, 2023. 2, 4, 6\n[37] Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain,\nPinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song.\nYou’ll Never Walk Alone: A Sketch and Text Duet for Fine-\nGrained Image Retrieval. In CVPR, 2024. 2\n[38] Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain,\nPinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song. How\nto Handle Sketch-Abstraction in Sketch-Based Image Re-\ntrieval? In CVPR, 2024. 2\n[39] Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain,\nPinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song. Text-\nto-Image Diffusion Models are Great Sketch-Photo Match-\nmakers. In CVPR, 2024. 2\n[40] Gihyun Kwon and Jong Chul Ye.\nDiffusion-based Image\nTranslation using Disentangled Style and Content Represen-\ntation. In ICLR, 2023. 5\n[41] Tuomas Kynk¨a¨anniemi, Tero Karras, Miika Aittala, Timo\nAila, and Jaakko Lehtinen. The Role of ImageNet Classes\nin Frech`et Inception Distance. In ICLR, 2023. 6\n[42] Lambda Labs. Stable Diffusion Image Variations, 2022. 4\n[43] Alexander C Li, Mihir Prabhudesai, Shivam Duggal, El-\nlis Brown, and Deepak Pathak.\nYour Diffusion Model\nis\nSecretly\na\nZero-Shot\nClassifier.\narXiv\npreprint\narXiv:2303.16203, 2023. 2\n[44] Changjian Li, Hao Pan, Adrien Bousseau, and Niloy J Mi-\ntra. Free2CAD: Parsing freehand drawings into CAD com-\nmands. ACM TOG, 2022. 2\n[45] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBLIP: Bootstrapping Language-Image Pre-training for Uni-\nfied Vision-Language Understanding and Generation.\nIn\nICML, 2022. 5, 6\n[46] Minchen Li, Alla Sheffer, Eitan Grinspun, and Nicholas Vin-\ning. Foldsketch: Enriching garments with physically repro-\nducible folds. ACM TOG, 2018. 2\n[47] Runtao Liu, Qian Yu, and Stella X Yu. Unsupervised Sketch-\nto-Photo Synthesis. In ECCV, 2020. 2\n[48] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay\nRegularization. In ICLR, 2019. 6\n[49] Yongyi Lu, Shangzhe Wu, Yu-Wing Tai, and Chi-Keung\nTang. Image Generation from Sketch Constraint Using Con-\ntextual GAN. In ECCV, 2018. 2\n[50] Ling Luo, Yulia Gryaditskaya, Tao Xiang, and Yi-Zhe Song.\nStructure-Aware 3D VR Sketch to 3D Shape Retrieval. In\n3DV, 2022. 2\n[51] Ling Luo, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song,\nand Yulia Gryaditskaya. 3D VR Sketch Guided 3D Shape\nPrototyping and Exploration. In ICCV, 2023. 2\n[52] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun\nWu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided Im-\nage Synthesis and Editing with Stochastic Differential Equa-\ntions. In ICLR, 2021. 2, 3\n[53] Qiang Meng, Shichao Zhao, Zhida Huang, and Feng Zhou.\nMagFace: A Universal Representation for Face Recognition\nand Quality Assessment. In CVPR, 2021. 5, 6\n[54] Aryan Mikaeili, Or Perel, Daniel Cohen-Or, and Ali\nMahdavi-Amiri. SKED: Sketch-guided Text-based 3D Edit-\ning. In CVPR, 2023. 2\n[55] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2I-Adapter: Learning\nAdapters to Dig out More Controllable Ability for Text-to-\nImage Diffusion Models. arXiv preprint arXiv:2302.08453,\n2023. 1, 2, 3, 4, 5, 6, 7, 12\n[56] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\nTransferable Visual Models From Natural Language Super-\nvision. In ICML, 2021. 3, 4, 5, 6\n[57] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-Shot Text-to-Image Generation. In ICML, 2021. 2\n[58] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical Text-Conditional Image Gen-\neration with CLIP Latents. arXiv preprint arXiv:2204.06125,\n2022. 3\n[59] Ren´e Ranftl,\nKatrin Lasinger,\nDavid Hafner,\nKonrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. IEEE TPAMI, 2022. 5\n[60] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan,\nYaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding in\nStyle: a StyleGAN Encoder for Image-to-Image Translation.\nIn CVPR, 2021. 2\n[61] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer.\nHigh-Resolution Image\nSynthesis with Latent Diffusion Models. In CVPR, 2022. 1,\n3, 4, 5, 6, 7, 8\n[62] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nNet: Convolutional Networks for Biomedical Image Seg-\nmentation. In MICCAI, 2015. 3\n[63] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. DreamBooth: Fine\nTuning Text-to-Image Diffusion Models for Subject-Driven\nGeneration. In CVPR, 2023. 2\n[64] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic Text-to-Image Diffusion Models with\nDeep Language Understanding. In NeurIPS, 2022. 1, 2, 3\n[65] Aneeshan Sain, Ayan Kumar Bhunia, Yongxin Yang, Tao Xi-\nang, and Yi-Zhe Song. StyleMeUp: Towards Style-Agnostic\nSketch-Based Image Retrieval. In CVPR, 2021. 2, 3, 5\n[66] Aneeshan Sain, Ayan Kumar Bhunia, Pinaki Nath Chowd-\nhury, Subhadeep Koley, Tao Xiang, and Yi-Zhe Song. CLIP\nfor All Things Zero-Shot Sketch-Based Image Retrieval,\nFine-Grained or Not. In CVPR, 2023. 2, 5, 6\n10\n[67] Aneeshan Sain, Ayan Kumar Bhunia, Subhadeep Koley,\nPinaki Nath Chowdhury, Soumitri Chattopadhyay, Tao Xi-\nang, and Yi-Zhe Song.\nExploiting Unlabelled Photos for\nStronger Fine-Grained SBIR. In CVPR, 2023. 2, 5\n[68] Kuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li,\nChen-Yu Lee, Kate Saenko, and Tomas Pfister. Pic2Word:\nMapping Pictures to Words for Zero-shot Composed Image\nRetrieval. In CVPR, 2023. 8\n[69] Patsorn Sangkloy, Nathan Burnell, Cusuh Ham, and James\nHays. The sketchy database: learning to retrieve badly drawn\nbunnies. ACM TOG, 2016. 5, 6, 7, 19\n[70] Patsorn Sangkloy, Wittawat Jitkrittum, Diyi Yang, and James\nHays. A Sketch Is Worth a Thousand Words: Image Re-\ntrieval with Text and Sketch. In ECCV, 2022. 1, 5\n[71] Idan Schwartz, V´esteinn Snæbjarnarson, Hila Chefer, Serge\nBelongie, Lior Wolf, and Sagie Benaim.\nDiscriminative\nClass Tokens for Text-to-Image Diffusion Models. In ICCV,\n2023. 2, 3\n[72] Jiaming Shen, Kun Hu, Wei Bao, Chang Wen Chen, and\nZhiyong Wang. Bridging the Gap: Fine-to-Coarse Sketch\nInterpolation Network for High-Quality Animation Sketch\nInbetweening. arXiv preprint arXiv:2308.13273, 2023. 2\n[73] Harrison Jesse Smith, Qingyuan Zheng, Yifei Li, Somya\nJain, and Jessica K Hodgins. A Method for Animating Chil-\ndren’s Drawings of the Human Figure. ACM TOG, 2023.\n2\n[74] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep Unsupervised Learning using\nNonequilibrium Thermodynamics. In ICML, 2015. 2\n[75] Jifei Song, Yi-Zhe Song, Tony Xiang, and Timothy M\nHospedales. Fine-Grained Image Retrieval: the Text/Sketch\nInput Dilemma. In BMVC, 2017. 5\n[76] Zhuo Su, Wenzhe Liu, Zitong Yu, Dewen Hu, Qing Liao, Qi\nTian, Matti Pietik¨ainen, and Li Liu. Pixel Difference Net-\nworks for Efficient Edge Detection. In ICCV, 2021. 4\n[77] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the Inception Ar-\nchitecture for Computer Vision. In CVPR, 2016. 6\n[78] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng\nPhoo, and Bharath Hariharan.\nEmergent Correspondence\nfrom Image Diffusion.\narXiv preprint arXiv:2306.03881,\n2023. 2\n[79] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-Play Diffusion Features for Text-Driven\nImage-to-Image Translation. In CVPR, 2023. 2, 3\n[80] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is All you Need. In NeurIPS, 2017. 4\n[81] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or.\nSketch-Guided Text-to-Image Diffusion Models.\nIn ACM\nSIGGRAPH, 2023. 1, 2, 3, 4, 6\n[82] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong\nChen, Qifeng Chen, and Fang Wen.\nPretraining is All\nYou Need for Image-to-Image Translation. arXiv preprint\narXiv:2205.12952, 2022. 2, 3, 6, 7, 12\n[83] Saining Xie and Zhuowen Tu. Holistically-Nested Edge De-\ntection. In ICCV, 2015. 4\n[84] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-\nlong Wang, and Shalini De Mello. Open-Vocabulary Panop-\ntic Segmentation with Text-to-Image Diffusion Models. In\nCVPR, 2023. 2\n[85] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xue-\njin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint\nby Example: Exemplar-based Image Editing with Diffusion\nModels. In CVPR, 2023. 4, 5\n[86] Lan Yang, Kaiyue Pang, Honggang Zhang, and Yi-Zhe Song.\nSketchAA: Abstract Representation for Abstract Sketches.\nIn ICCV, 2021. 4\n[87] Lan Yang, Kaiyue Pang, Honggang Zhang, and Yi-Zhe Song.\nFinding Badly Drawn Bunnies. In CVPR, 2022. 2, 5\n[88] Emilie Yu, Rahul Arora, J Andreas Baerentzen, Karan Singh,\nand Adrien Bousseau. Piecewise-smooth surface fitting onto\nunstructured 3D sketches. ACM TOG, 2022. 2\n[89] Qian Yu, Feng Liu, Yi-Zhe Song, Tao Xiang, Timothy M\nHospedales, and Chen-Change Loy. Sketch Me That Shoe.\nIn CVPR, 2016. 3\n[90] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nConditional Control to Text-to-Image Diffusion Models. In\nICCV, 2023. 1, 2, 3, 4, 6, 7, 12\n[91] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A\nEfros. Unpaired Image-to-Image Translation using Cycle-\nConsistent Adversarial Networks. In ICCV, 2017. 6, 12\n11\nSupplementary material for\nIt’s All About Your Sketch: Democratising Sketch Control in Diffusion Models\nSubhadeep Koley1,2\nAyan Kumar Bhunia1\nDeeptanshu Sekhri1\nAneeshan Sain1\nPinaki Nath Chowdhury1\nTao Xiang1,2\nYi-Zhe Song1,2\n1SketchX, CVSSP, University of Surrey, United Kingdom.\n2iFlyTek-Surrey Joint Research Centre on Artificial Intelligence.\n{s.koley, a.bhunia, d.sekhri, a.sain, p.chowdhury, t.xiang, y.song}@surrey.ac.uk\nA. Additional Qualitative Results\nFig. 13 delineates qualitative comparison of our method with pix2pix [30], CycleGAN [91], ControlNet [90], T2I-Adapter\n[55], and PITI [82]. Whereas, Fig. 14-19 shows additional results generated by our method.\nSketch\nOurs\npix2pix\nCycleGAN\nControlNet\nT2I-Adapter\nPITI\nFigure 13.\nQualitative comparison with SOTAs.\nFor ControlNet [90], T2I-Adapter [55], and PITI [82], we use the fixed prompt\n“a photo of [CLASS]”, with [CLASS] replaced with corresponding class-labels of the input sketches. (Best view when zoomed in.)\n12\nFigure 14. Images generated by our method.\n13\nFigure 15. Images generated by our method.\n14\nFigure 16. Images generated by our method.\n15\nFigure 17. Images generated by our method.\n16\nFigure 18. Images generated by our method.\n17\nFigure 19. Images generated by our method.\n18\nB. Results on Out-of-distribution Sketches\nKeeping the pre-trained diffusion model frozen, we fully leverage its generalisation potential. We posit that our design enables\nout-of-distribution generalisation. Fig. 20 shows a few sketches, absent in Sketchy [69].\nFigure 20. Results on out-of-distribution sketches.\n19\n"
    },
    {
        "Published": "2024-01-23",
        "Title": "No AI After Auschwitz? Bridging AI and Memory Ethics in the Context of Information Retrieval of Genocide-Related Information",
        "Authors": "Mykola Makhortykh",
        "Summary": "The growing application of artificial intelligence (AI) in the field of\ninformation retrieval (IR) affects different domains, including cultural\nheritage. By facilitating organisation and retrieval of large volumes of\nheritage-related content, AI-driven IR systems inform users about a broad range\nof historical phenomena, including genocides (e.g. the Holocaust). However, it\nis currently unclear to what degree IR systems are capable of dealing with\nmultiple ethical challenges associated with the curation of genocide-related\ninformation. To address this question, this chapter provides an overview of\nethical challenges associated with the human curation of genocide-related\ninformation using a three-part framework inspired by Belmont criteria (i.e.\ncuration challenges associated with respect for individuals, beneficence and\njustice/fairness). Then, the chapter discusses to what degree the\nabove-mentioned challenges are applicable to the ways in which AI-driven IR\nsystems deal with genocide-related information and what can be the potential\nways of bridging AI and memory ethics in this context.",
        "Page_content": "1\nThis is a pre-print version of the chapter published in Ethics in Artificial Intelligence:\nBias, Fairness and Beyond. The final authenticated version is available online at\nhttps://link.springer.com/chapter/10.1007/978-981-99-7184-8_4.\nTo cite this chapter:\nMakhortykh M. (2023). No AI After Auschwitz? Bridging AI and Memory Ethics in the\nContext of Information Retrieval of Genocide-Related Information. In A. Mukherjee, J.\nKulshrestha, A. Chakraborty, and S. Kumar (Eds.), Ethics in Artificial Intelligence: Bias,\nFairness and Beyond (pp. 71-85). Springer. DOI:\nhttps://doi.org/10.1007/978-981-99-7184-8_4.\n2\nNo AI After Auschwitz? Bridging AI and Memory Ethics in the Context of Information\nRetrieval of Genocide-Related Information\nMykola Makhortykh1\nAbstract: The growing application of artificial intelligence (AI) in the field of information\nretrieval (IR) affects different domains, including cultural heritage. By facilitating organisation\nand retrieval of large volumes of heritage-related content, AI-driven IR systems inform users\nabout a broad range of historical phenomena, including genocides (e.g. the Holocaust).\nHowever, it is currently unclear to what degree IR systems are capable of dealing with\nmultiple ethical challenges associated with the curation of genocide-related information. To\naddress this question, this chapter provides an overview of ethical challenges associated\nwith the human curation of genocide-related information using a three-part framework\ninspired by Belmont criteria (i.e. curation challenges associated with respect for individuals,\nbeneficence\nand justice/fairness). Then, the chapter discusses to what degree the\nabove-mentioned challenges are applicable to the ways in which AI-driven IR systems deal\nwith genocide-related information and what can be the potential ways of bridging AI and\nmemory ethics in this context.\n1 Introduction\nInformation retrieval (IR) is one of the computer science fields that is closely connected to\nthe developments in the domain of artificial intelligence (AI). Defined as the process of\nselecting items that are deemed relevant for the user information needs based on the user\ninput [1], IR has been argued to be a particularly promising area of applying AI [2, 3]. The\nintegration of AI can benefit different aspects of IR, ranging from knowledge representation\nto content indexing and matching [6] to relevance modelling [3, 4]. Consequently, there is a\nlong history of research on AI-driven IR applications, starting with rule-based approaches in\nthe 1980s [2] and ending with the neutral network-based approaches discussed in the 2020s\n[4].\nThe importance of AI-driven IR systems has been increasing due to the growth in the\namount of information available online. Often referred to as an information overload [5], this\nphenomenon prompted the need for advanced IR mechanisms for satisfying individual\ninformation needs, which are capable of not only processing the large volumes of available\ninformation but also recognising the diverse spectrum of user needs and in some cases\n1 University of Bern, Switzerland\n3\npredicting these needs. Such mechanisms demonstrated their usefulness in multiple\ndomains ranging from healthcare [7, 8] to journalism [9, 10] to e-commerce [11, 12].\nThis chapter focuses on one particular domain in which AI-driven IR systems are\nincreasingly employed, which is cultural heritage. By facilitating the organisation of\nheritage-related content both within heritage institutions (e.g. archives [13] or museums\n[14]), and commercial platforms (e.g. web search engines [15] or social media news feeds\n[16]), AI-driven IR systems help their users become informed about a broad range of\nhistorical phenomena, including genocides such as the Holocaust or Rwanda genocide.\nUnder the condition of a high degree of autonomy, these systems become non-human\ncurators of genocide-related information, which shape how individuals and societies are\ninformed about the past and present atrocities.\nDespite the importance of IR systems for curating information about historical and\nrecent genocides, there are multiple concerns about their potential impact on genocide\nremembrance. The usual concerns about the lack of transparency of AI-driven IR systems\nare further amplified by the possibility of such non-transparency facilitating manipulations of\nIR systems, which can potentially interfere with the moral obligations of safeguarding the\ndignity of genocide victims [17]. Furthermore, such manipulations can facilitate the\ninstrumentalisation of memories about past violence, which can be used for justifying the\npresent stigmatisation, as in the case of the Rohingya persecution in Myanmar [18] or the\nRussian-Ukrainian war [19].\nBesides\nthe\nabove-mentioned\nconcerns\nabout\nthe\nuse\nof\nIR\nfor\ncurating\ngenocide-related information, there are also other ethical challenges which have for long\nbeen discussed in the context of human curation of historical information, such as the\nimportance of protecting the privacy of individuals [20] or preventing unfair practices of\ninformation curation [21]. However, despite the growing body of work concerning the ethics\nof human curation of genocide-related information [22–24], the capabilities of IR systems to\ndeal with complex ethical issues arising in the context of genocide-related information as\nwell as the perspectives of bridging memory ethics and IR design currently remain\nunder-investigated.\nTo address this gap, the chapter aims to examine whether the concerns about the\nhuman curation of genocide-related information are applicable to AI-driven IR systems and\nhow these concerns can potentially be addressed. For this aim, it provides a short overview\nof the current applications of IR systems in the context of genocide-related information,\nfollowed by a discussion of the ethical challenges of its human curation using a three-part\nframework inspired by Belmont criteria (i.e. respect for individuals, beneficence and\njustice/fairness). Finally, the chapter discusses to what degree these challenges are\n4\napplicable to AI-driven IR systems and what can be the potential ways of bridging AI and\nmemory ethics in this context.\n2 AI-driven IR Systems and Genocide-Related Information\nThe digitisation of historical collections, together with the production of new digital-born\nmaterials dealing with information about genocides (e.g. audiovisual tributes to the\nHolocaust [25, 26]), prompted the growing use of IR systems within heritage institutions.\nMany of these systems partially reproduce or enhance the traditional curation practices used\nby archives or museums, but in some cases, IR systems substantially transform the scale\nand functionality of these practices.\nFor instance, Liew [14] examined how IR systems facilitate the exploration of\ncollections, including the ones dealing with the Holocaust (e.g. by enabling keyword/phrase\nsearch and the use of wildcard operators). Schenkolewski-Kroll and Tractinsky [13]\ndiscussed the relationship between IR systems and authority lists in the context of Holocaust\nmaterials in the Israeli archives. Daelen [27] looked at the possibilities of using IR to enrich\nthe inventory of collections related to the Holocaust and connect archives and users in the\ncontext of European Holocaust Research Infrastructure; similarly, Carter et al. [28] discussed\nthe potential of AI-driven IR solutions for facilitating exploration of primary sources in the\ncontext of the Holocaust by proving new possibilities for user interaction with the Morgenthau\nDiaries.\nIn addition to IR systems enhancing traditional curation practices, there are also\nexamples of more innovative applications of these systems within heritage institutions. One\nexample is the use of three-dimensional visualisation of Holocaust survivors retrieving audio\nrecordings of the survivors’ earlier comments in response to the user input. Sometimes\nreferred to as holograms [29] or social robots [30], these systems are used by several\nHolocaust memory initiatives (e.g. New Dimensions of Testimony or Forever Project) and\nenable new possibilities to retrieve testimonies through the simulation of human-to-human\nconversation.\nSimilar to other experimental proposals concerning the use of AI-driven IR systems in\nthe\ncontext\nof\ngenocide\nremembrance\n(e.g.\nthe\nconcept\nof\npersonalised\nvirtual\nreality-enhanced interaction with information about the Holocaust for the Babyn Yar\nmemorial [31]), the use of social robots as a form of curation of genocide-related information\nhas attracted not only praise but also criticism. For instance, Walden [29] noted that novel\napproaches for the use of IR (e.g. in the form of Holocaust survivors’ holograms) do not\nnecessarily meet the expectations about reactualisation of the past for the audience,\nwhereas Alexander [32] noted that some of these novel approaches require not only\n5\nhistorical knowledge but also media literacy, thus risking to make information less accessible\nfor certain groups.\nNot only heritage institutions but also commercial platforms are increasingly relying\non IR systems for curating genocide-related information. The availability of digital content\nrelated to genocides coming both from the institutional (e.g.Holocaust museums [33]) as well\nas non-institutional entities such as online influencers [34] or artists [25] resulted in the\ngrowing presence of genocide-related information on the online platforms. These platforms\nrange from social media sites, such as Instagram [34] or TikTok [35], to web search engines,\nsuch as Google [36], to commerce-related platforms (e.g. TripAdvisor [37]).\nUnder\nthese\ncircumstances,\nIR\nsystems\nbecome\na\ncrucial\nelement\nof\ngenocide-related information curation, in particular, considering that commercial platforms\noften lack human curation expertise in this specific domain that differentiates them from\nheritage institutions. However, the implications of IR-driven curation currently remain unclear.\nMakhortykh et al. [36] examined how six web search engines curate visual information about\nthe Holocaust and observed substantial differences in what aspects of the Holocaust are\nprioritised by the individual engines. Devon and Tobias-Hartmann [35] discussed the impact\nof the TikTok algorithm on the treatment of user-generated content, including content dealing\nwith Holocaust denial, and found the tendency of the algorithm to suppress certain forms of\nresistance to antisemitic ideologies. Finally, Kansteiner [38] looked at how Holocaust\ninstitutions use IR systems associated with commercial social media sites (e.g. Facebook)\nand found the varying degrees of visibility of specific types of genocide-related content\nreceived.\nUnsurprisingly, the use of AI-driven IR systems by commercial platforms for curating\ninformation about genocides also raised a number of concerns. In addition to the general\ncritique of it undermining the gatekeeping functions of heritage institutions [39], studies\nsuggest that IR systems used by platforms can promote factually incorrect or denialist\ncontent [36, 40]. Another concern relates to the possibility of commercial platforms’ IR\nsystems resulting in unequal treatment of information about different aspects of specific\ngenocides (e.g. in terms of prioritising content coming from a few Holocaust sites while\nomitting the other ones [36]).\n3 Memory Ethics and Human Curation of Genocide-Related Information\nThe major challenge of bridging AI and memory ethics in the context of genocide-related\ninformation curation deals with the multiple forms the curation might take. There are many\napproaches to human curation of such information, ranging from the one happening in\n6\nheritage-focused environments, for instance, museums or archives, to wider public-focused\nenvironments, such as mass media.\nThe multiplicity of forms of curation prompts the importance of identifying which of\nthem are particularly applicable to the discussion of AI-driven IR systems. While it can be\ndebated, the argument can be made that human curation in archives is the closest in its\nnature. Both archives and IR systems determine what content is made visible to the public\nand what content remains hidden [23]: in the case of archives, these decisions are\nimplemented by providing or not providing physical access to the collections, whereas in the\ncase of IR systems, some outputs in response to user queries can be filtered out or down\nranked.\nHuman curation of genocide-related information in the archival context has to deal\nwith multiple ethical challenges [21, 24]. Some of these challenges are applicable to archival\nresearch in general, for instance, the potential damages to individual privacy [23]. However,\nother challenges are more specific to the case of genocide and include, for instance, the\npossibility of using archives to subjugate knowledge about the past atrocities [41] or\nimpeding the processing of genocide-related trauma by encouraging specific types of\ntestimonies and silencing others [42]. The need to address these challenges stimulates the\ndiscussion of how these ethical challenges can be addressed.\nOne of the common reference points in the discussion of ethics regarding human\ncuration of archival information is Belmont criteria. Introduced at the end of the 1970s to\nprovide guidelines for research involving human subjects, Belmont’s criteria focus on three\nethical principles: respect for individuals, beneficence, and justice (sometimes also referred\nto as fairness [24]). The recommendations of Belmont criteria generally suggest that\n“informed consent be sought, that benefits and risks be evaluated, and the selection,\nrepresentation, and the burden of participation be fair and equitable” [43, p. 139].\nA number of studies have critically interrogated to what degree Belmont criteria are\napplicable for archival research [22, 24]. Some studies argued that Belmont criteria are not\napplicable to archival research because it is fundamentally different from the other\ndisciplines working with human subjects [23], whereas others (e.g. [24]) suggested that the\ncriteria are focused primarily on preventing potential damage for the living subjects.\nHowever, in the case of genocide-centred research, many subjects are already dead that\nmakes it hardly possible to obtain their consent for being involved in the research and the\ndifferent set of risks/threats (e.g. potential damage to posthumous dignity of victims [17])\nwhich have implications for the beneficence and justice criteria. Under these conditions,\ndirect application of Belmont criteria to archival research dealing with genocides may\nundermine the ethical mandate of the genocide-focused scholarship [22].\n7\nDespite the above-mentioned drawbacks, it can be argued that Belmont criteria are\nstill applicable for identifying the ethical challenges involved in the human curation of archival\ninformation about genocides. Specifically, this chapter proposes to apply a three-part\nframework\nusing\nBelmont\ncriteria—i.e.\nrespect\nfor\nindividuals,\nbeneficence\nand\njustice/fairness—to\ngroup\ntogether\npotential\nethical\nchallenges\nassociated\nwith\ngenocide-related information curation. The rest of the section is devoted to the discussion of\nthe individual challenges associated with each of the three criteria.\nIn the case of respect for individuals, it is possible to identify three major ethical\nchallenges related to the human curation of genocide-related information: consent, double\nvision, and privacy. The first of these challenges—i.e. the need to acquire consent—is\ncommon for curation of information coming from human subjects in other contexts. However,\nin the case of genocide or other forms of mass violence, making sure that consent is\nacquired becomes a much harder task. In some cases, the difficulties can be due to\nsubstantial risks for witnesses or victims preventing them from voluntarily sharing information\n[44] or evidence being produced against the will of the victims [45].\nFurthermore,\nthe\ndigitisation\nof\ngenocide-related\ninformation\nraises additional\nquestions such as, for instance, whether the consent given for the generation of analogue\nmaterials also automatically applies to their digitalisation and whether the difference between\nanalogue and digital public access has implications for the consent to make information\nabout the genocide publicly available [20]. These questions are particularly applicable for the\nhistorical instances of genocide (e.g. the Holocaust), where materials (e.g. testimonies) were\nproduced in certain formats which have since then become outdated, so preserving them in\nthe original format is both non-sustainable and ineffective from the point of view of\ncommunicating information about the genocide.\nAnother challenge of human curation relates to the problem of double vision, which\nrelates to the transformation of genocide victims into objects (and not subjects) of research\ndue to the distancing involved in the process of data collection and analysis [46]. Originally\ndiscussed in the context of processing analogue materials [22, 46], the problem of potential\ndehumanisation and depersonalisation of victims is amplified by the shift towards digital\ncollections, enabling new possibilities for “anonymizing, numbering, and classifying” [24, p.\n531] experiences of genocide victims as well as “converting humans into numbers” [47, p.\n322].\nOne more aspect of respect for individuals concerns the matters of privacy. Archives,\nin general, can be damaging to the reputation of individuals whose information is disclosed\nwithout their consent [20, 23]. However, in the case of genocide, in particular its recent\ninstances, privacy can be a matter of life and death, for instance, when either perpetrators or\nvictims want to take revenge in their hands. At the same time, the profound anonymisation of\n8\ngenocide-related records has been criticised for its potential for erasing the voice of victims\n[22], which in some way is similar to the purpose of the genocidal actions aiming to erase\nany traces of victims.\nFor the beneficence of the human curation of genocide-related information, it is\npossible to identify two major challenges: the problem of representation and the possibility of\ndistortion/manipulation.\nThe\nformer challenge relates to the argument that because\ngenocides are instances of unprecedented violence, any attempt of their representation (e.g.\nvia certain modes of storytelling or documentation [30, 48]) is inadequate. Hence, the\nabsence of representation might be a “more accurate or truthful or morally responsive” [49,\np. 71] way of dealing with genocide-related information.\nThe second challenge concerns the possibility of information about genocide being\ndistorted or manipulated. The forms of distortion of historical information can vary broadly;\nsome examples include de-contextualisation of historical phenomena [20], denial or\njustification of the past crimes [50] or the use of references to past suffering or injustice for\nstigmatising specific social groups in the present [19]. In the case of genocide-related\ninformation, such forms of distortion are particularly concerning both due to the ethical\nobligations of protecting the memory of victims and the strong affective potential of\ninformation about past injustices, which can be used to incite violence in the present [26].\nFinally, the justice/fairness of human curation concerns two interrelated aspects: the\npoliticisation of curation and the unequal treatment of specific types of genocide-related\ninformation. One of them is the politicisation of archives, which has implications for what\ninformation about the past atrocities is available and how it is communicated to the public\n[24, 47]. The transmission of the matters of curation of genocide-related information to the\nrealm of politics might not only downplay the importance of ethical obligations associated\nwith it but also facilitate instrumentalisation of genocide memory for immediate political\ngains. Such instrumentalisation can lead to genocide-related information being used to\nmanipulate public opinion, for instance, to justify violence in the present, as it happened in\nthe case of the Russian aggression against Ukraine.\nThe second justice-related challenge deals with the unequal treatment of certain\ntypes of genocide-related information. In some cases, it is attributed to politicisation of\narchives, which can lead to the release of information (e.g. in the form of archival\ndocuments) supporting certain political agendas [47] or silencing of information which can be\nviewed as damaging for a ruling regime [41]. In other cases, the unequal treatment can be\nrelated to the belief that some types of information can be less reliable (e.g. due to the\nassumption that genocide victims can not provide a neutral view on the genocide [21]) or the\nimbalance between the availability of different types of information (e.g. because of certain\ngroups of individuals being more likely to survive and, thus, leave testimonies [47]).\n9\n4 Bridging Memory Ethics and AI-driven IR System Design\nAfter identifying the common ethical challenges of human curation of genocide-related\ninformation, it is important to examine to what degree they are applicable for IR\nsystem-based curation and how IR system design can be bridged with memory ethics to\naddress concerns associated with these challenges. For this purpose, this section will use\nthe same framework of three groups of challenges related to respect for individuals (consent,\ndouble vision, and privacy), beneficence (representation and distortion/manipulation), and\nfairness/justice (politicisation of curation and unequal treatment).\nIn the case of consent, the difficult part of bridging ethics and IR design relates to\nconsent granting usually being part of the initial stage of data generation (e.g. the recording\nof a testimony or registering of an account to upload digital materials). One exception here\nrelates to IR systems used in the context of web search, where indexing of digital-born\nmaterials is an ongoing process. However, in most cases, the IR systems process data for\nwhich consent has already been granted (e.g. in the case of collections stored in the\nheritage institution or materials generated through the online platform). While it can be\npossible to integrate consent checks or regular requests for consent re-granting, this specific\naspect\ncan\narguably\nbe more relevant for the overall model of institution/platform\nfunctionality and not necessarily for the IR design.\nIn terms of double vision, AI-driven IR systems are sometimes argued [30] to be\ncapable of encouraging trust and empathy, which can counter the potential dehumanisation\nof victims associated with this challenge. Such a problem can be particularly pressing with\nthe passing of the living witnesses of the genocides, which amplifies the risk of them being\nincreasingly treated as objects and not the subjects of research. One particular example of\nthe use of IR for countering this issue is the shift towards more human-to-human\ncommunication-like forms of IR, for instance, the use of conversational agents as a form of\ncuration of genocide-related information.\nThe potential of IR systems for dealing with privacy-related challenges shares certain\nsimilarities with the case of content. In some cases, the matters of privacy are dealt with\nduring the initial state of data generation (e.g. in the case of thorough anonymisation of\ngenocide-related evidence). However, in other cases, IR systems can have a rather\nambiguous impact on the privacy of individuals the information about whom they are\ncurating. Advancements in several fields of AI (e.g. computer vision and natural language\nprocessing; [63]) enable novel possibilities for recognising the presence of individuals or\nmentions of specific entities in the data; however, the same advancements can be used for\nprotecting individual privacy (e.g. by masking private information present in the documents).\n10\nUnder these circumstances, the inclusion of particular functionalities in the IR system design\ncan either expose or protect private information. The choice of functionalities can be\ninformed by examining the selection of materials which the system is expected to work with\nand its intended uses.\nFiltering\nout\nprivacy-sensitive\ncontent\ncan\nbe\nanother\nalternative\nto making\nmodifications to the original data. Often associated with the right to be forgotten [58], also\nknown as the right to erasure, this approach can be particularly applicable in the case of IR\nsystems dealing with genocide-related information in the context of web search, where the\nmodification of indexed data is not necessarily possible. Applied for protecting individual\nprivacy, this mechanism might be less applicable for genocide-related information, where its\nevocation for specific cases (e.g. the application of the right to be forgotten for perpetrators\nof genocide) might contradict the public interest [59]. However, in other cases (e.g.\nprotecting the victims), it might be essential for tackling privacy-related challenges, thus\nstressing the importance of functionalities that can facilitate requests for the activation of the\nright to be forgotten in the IR system design.\nFrom the point of view of beneficence-related challenges, IR can enable new\npossibilities for addressing the problem of representation. The new formats of curating\ngenocide-related information (e.g. via social bots [30]), as well as more personalised\napproaches (e.g. the ones taking into consideration the level of knowledge identified on the\nbasis of earlier history of interactions with the IR system), can move the genocide-focused\nstorytelling beyond the traditional modes of representation. While the adequacy of these\nnovel IR approaches for dealing with genocide-related information can be debated, it is\nimportant to investigate their potential.\nSimilar to addressing the problem of representation, AI-driven IR systems can\nfacilitate countering distortion of genocide-related information. The possible approaches for\ndoing it vary from automated detection of distorted information (e.g. the denialist claims) and\ntheir subsequent filtering/de-prioritisation (e.g. in the case of Holocaust denial content being\ncountered by commercial platforms) to the provision of contextual information to the system\noutputs dealing with the genocide. The growing body of research on integrating mechanisms\nof detecting and countering misinformation in AI-driven IR systems [60, 61] demonstrates\npossibilities provided by these systems for preventing the distortion of historical facts.\nAt the same time, it is important to acknowledge the dangers posed by IR systems to\nthe beneficence of curation of genocide-related information, in particular, in the context of the\nincreasing complexity of IR systems [30]. Such complexity makes it more difficult to identify\npotential instances of system manipulation, in particular, for the users having a limited\nunderstanding of the logic behind the system functionality. Together with the limited\nknowledge about the overall composition of the pool of outputs (e.g. in the case of web\n11\nsearch IR systems dealing potentially with billions of genocide-related outputs), it stresses\nthe importance of integrating transparency in the IR system design.\nIn the case of concerns about archives’ politicisation, the impact of AI-driven IR\nsystems can be ambiguous. Depending on how aware of the functionality of IR systems\nactors involved in politicisation of genocide-related information are and to what degree these\nactors are capable of influencing the system, IR systems can either facilitate politicisation or\ncounter it. Contextual factors are particularly important for instance, under the condition of\nintense\npoliticisation\nof\ngenocide-related information within a particular country, the\ntransparent functionality of IR systems used by the local heritage institutions may actually\nfacilitate the appropriation of the systems for controlling information curation. By contrast,\nnon-transparent IR mechanisms used by a transnational company that is less dependent on\nthe whims of the local memory regime may actually counter politicisation by offering a less\npoliticised selection of information.\nThe question of the intended use of the IR systems is also of particular importance\nfor identifying their ability to deal with unequal treatment of genocide-related information.\nSimilar to IR systems dealing with news [62], genocide-focused IR systems can serve\ndifferent normative functions. More deliberative models of IR systems can be optimised for\nenabling equal representation of genocide-related information (e.g. in terms of visibility of\nspecific aspects of the genocide or particular sites [36]) via either personalised or\nnon-personalised curation, whereas more liberal models might omit the matters of equality,\ninstead giving visibility to a few prominent aspects which the system expects the user to be\nparticularly interested in. The preference for a particular model determines the logic behind\nthe design of a particular IR system; however, determining such a preference might be a\nrather non-trivial task (e.g. what stakeholder groups shall be able to decide on it?), which is\nalso true for realising more complex models of information curation (e.g. what characteristics\nto take into consideration when deciding on the equality/lack of equality in representation of\nspecific aspects of a genocide?).\n5 Discussion\nThe chapter scrutinised the ways for bridging AI and memory ethics in the context of IR\nsystems dealing with genocide-related information. Using a Belmont criteria-inspired\ntypology of ethical challenges associated with human curation of information about\ngenocides, it discussed to what degree IR systems can address curation issues related to\nrespect for individuals, beneficence and justice/fairness. The results of this discussion\nhighlight several important points concerning the potential of IR systems for curating\n12\ninformation about genocides, both historical (e.g. the Holocaust) and recent ones (e.g.\nRohingya genocide).\nThe first point suggests that AI-driven IR systems are, unfortunately, not a silver\nbullet\ncapable\nof\neasily\nsolving ethical challenges associated with the curation of\ngenocide-related information. Even while they can address some of the issues related to\nhuman curation (e.g. by enabling new possibilities for addressing some of respect- or\nfairness-related challenges), they can also worsen other issues (e.g. the beneficence-related\nchallenges), in particular, considering the high complexity and frequent lack of transparency\nof IR systems. Under these circumstances, it becomes of paramount importance to take into\nconsideration the complex relationship between IR systems and memory ethics when\ndesigning the former to minimise the possibility of IR having detrimental effects on the lives\nof individuals affected by genocides and on genocide remembrance.\nSecond, similar to other domains (e.g. journalism [51]), there is a tradeoff between\nthe realisation of AI potential for enhancing the performance of IR systems (e.g. in terms of\naddressing\nethical\nchallenges,\nin\nparticular,\nthe\nones\nrelated\nto\nbeneficence\nand\njustice/fairness) and transparency. While the increased complexity of IR systems enables\nnew possibilities for making the treatment of different groups of genocide victims more fair\n(e.g. in terms of making their suffering equally visible via AI curation) and dealing with the\nproblem of representation (e.g. in terms of filtering out and removing information distorting\nhistorical facts), it also makes the functionality of these systems less transparent, thus\nlimiting the user control over the system [52].\nThird, the growing presence of genocide-related information on commercial (and not\nonly heritage-oriented) platforms poses additional difficulties for its curation through IR\nsystems. Because of the generalist focus of commercial platforms (e.g. Google), it is difficult\n(albeit not impossible, as shown by the case of COVID-related information moderation [53])\nto enable distinct treatment of specific types of information. Under these circumstances, IR\nsystems used by commercial platforms often treat information about sensitive and traumatic\nsubjects (e.g. genocides) in the same way and follow the same logic (e.g. to maximise user\nengagement as in the case of some social media sites) as other subjects such as\nentertainment topics. The possibility of such non-differentiated treatment can result in a\nnumber of ethics-related issues (in particular, related to the respect for individuals and\nbeneficence of curation) and prompts the importance of the dialogue between the\ncommercial platforms and heritage practitioners as well as other genocide-related actors\n(e.g. survivors or their families) in order to find a way for addressing these issues.\nFinally, it is important to note several limitations of the conducted study. The primary\nlimitation is the reliance on a conceptual approach to discuss the relationship between IR\nsystems and memory ethics. Specifically, the chapter relies on the existing academic\n13\nscholarship for synthesising the main challenges of human curation of genocide-related\ninformation and discussing the possible ways of addressing them through using AI-driven IR\nsystems. Future research can benefit from a more empirically-driven approach (e.g. based\non interviews) to solicit opinions of heritage practitioners on the ethics-related issues\ninvolved in curation of information about different genocides as well as how these can be\naffected by IR systems.\nA related challenge concerns the focus on the existing research on memory ethics\nand information curation regarding one particular instance of genocide, namely the\nHolocaust. While such a focus is not surprising considering the particular importance of the\nHolocaust, in particular, for the Global North [54], it is crucial to acknowledge that information\nabout other genocides might pose different challenges, in particular considering the\nuniqueness of each genocide [55] as well as the increasing criticism of West-oriented\nstandardisation of genocide commemoration [56, 57]. Under these circumstances, it is\nimportant not only to extend the discussion of the role of IR systems to other instances of\ngenocide, including the ones occurring in the Global South and Global East, but take into\nconsideration that requirements for AI-driven IR systems in these cases may be different.\nReferences\n1. Salton G, McGill, MJ (1983) Introduction to modern information retrieval. Mcgraw-Hill.\n2.\nJones\nKS\n(1999)\nInformation\nretrieval\nand\nartificial\nintelligence.\nArtif\nIntell\n114(1–2):257–281.\n3. Boughanem M, Akermi I, Pasi G, Abdulahhad, K (2020) Information retrieval and artificial\nintelligence. In: A guided tour of artificial intelligence research. Springer, Cham.\n4. Guo J et al (2020) A deep look into neural ranking models for information retrieval. Inf\nProcess & Manag 57(6).\n5. Roetzel PG (2019) Information overload in the information age: a review of the literature\nfrom\nbusiness\nadministration,\nbusiness\npsychology,\nand\nrelated\ndisciplines\nwith\na\nbibliometric approach and framework development. Bus Res 12(2):479–522.\n6. Jones KS (1991) The role of artificial intelligence in information retrieval. J Am Soc Inf Sci\n42(8):558–565.\n7. Hersh WR (2015) Information retrieval for healthcare. In: Reddy C, Aggarwal C (eds)\nHealthcare data analytics. CRC Press, Boca Raton.\n8. Miranda A, Miah SJ (2021) Designing an innovative unified contextual architecture for\nimproving\ninformation\nretrieval\nservice\nin\nhealthcare\norganizations.\nInf\nDev.\nhttps://doi.org/10.1177/0266666921104949.\n14\n9. Karimi M, Jannach D, Jugovac M (2018) News recommender systems-Survey and roads\nahead. Inf Process & Manag 54(6):1203–1227.\n10. Mitova E, Blassnig S, Strikovic E, Urman A, Hannak A, de Vreese CH, Esser F (2022)\nNews\nrecommender\nsystems:\na\nprogrammatic\nresearch\nreview.\nAnn\nInt\nCommun\nAssoc:1–30.\n11. Alamdari PM, Navimipour NJ, Hosseinzadeh M, Safaei AA, Darwesh A (2020) A\nsystematic\nstudy\non\nthe\nrecommender\nsystems\nin the E-commerce. IEEE Access\n8:115694–115716.\n12. Zhang H et al (2020) Towards personalized and semantic retrieval: an end-to-end\nsolution for e-commerce search via embedding learning. In: Proceedings of the 43rd\ninternational ACM SIGIR conference on research and development in information retrieval.\nACM, New York.\n13. Schenkolewski-Kroll S, Tractinsky A (2006) Archival description, information retrieval,\nand the construction of thesauri in Israeli archives. Arch Sci 6(1):69–107.\n14. Liew CL (2005) Online cultural heritage exhibitions: a survey of information retrieval\nfeatures. Program 39(1):4–24.\n15. Jakubowicz A (2009) Remembering and recovering Shanghai: Seven Jewish families\n[re]-connect in cyberspace. In: Save as. . . digital memories. Palgrave Macmillan, London.\n16. Santana Talavera A, Rodríguez Darias AJ, Díaz Rodríguez P, Aguilera Ávila L (2012)\nFacebook, heritage and tourism reorientation. The cases of Tenerife and Fuerteventura\n(Canary Isles, Spain). Int J Web Based Communities 8(1):24–39.\n17. Lechtholz-Zey J (2012) The laws banning Holocaust denial. Genocide Prevention Now 9.\nAvailable via Institute on the Holocaust & Genocide in Jerusalem. https://cutt.ly/M1bdkbj.\nCited 28 Nov 2022.\n18. Ware A, Laoutides C (2019) Myanmar’s ‘Rohingya’ conflict: Misconceptions and\ncomplexity. Asian Aff 50(1):60–79.\n19. Makhortykh M (2018) NoKievNazi: social media, historical memory and securitization in\nthe Ukraine crisis. In: Memory and securitization in contemporary Europe. Palgrave\nMacmillan, London.\n20. Agarwal K (2016) Doing right online: archivists shape an ethics for the digital age.\nAvailable via Perspectives on History. https://cutt.ly/r1bfwQ8. Cited 28 Nov 2022.\n21. Altanian M (2017) Archives against genocide denialism?. Available via Swisspeace.\nhttps://cutt.ly/n1bfxg6. Cited 28 Nov 2022.\n22. Einwohner RL (2011) Ethical considerations on the use of archived testimonies in\nHolocaust research: Beyond the IRB exemption. Qual Sociol 34(3):415–430.\n23. Tesar M (2015) Ethics and truth in archival research. Hist Educ 44(1):101–114.\n15\n24. Subotic J (2021) Ethics of archival research on political violence. J Peace Res\n58(3):342–354.\n25. Gibson PL, Jones S (2012) Remediation and Remembrance:’Dancing Auschwitz’\nCollective Memory and New Media. J Commun Stud 5(10):107–131.\n26. Makhortykh M (2019) Nurturing the pain: audiovisual tributes to the Holocaust on\nYouTube. Holocaust Stud 25(4):441–466.\n27. Daelen VV (2019) Data sharing, holocaust documentation and the digital humanities:\nintroducing the European holocaust research infrastructure (EHRI). Umanistica Digitale\n3(4):1–9.\n28. Carter KS, Gondek A, Underwood W, Randby T, Marciano R (2022) Using AI and ML to\noptimize information discovery in under-utilized Holocaust-related records. AI&Soc 37:837–\n858.\n29. Walden VG (2022) What is ‘virtual Holocaust memory’? Mem Stud 15(4):621–633.\n30. Shur-Ofry M, Pessach G (2019) Robotic collective memory. Wash Univ Law Rev\n97:975–1005.\n31. Pravda I (2020) Babyn Yar. The museum of horrors directed by Khrzhanovsky. Available\nvia Istorychna pravda. https://www.istpravda.com.ua/eng/articles/2020/05/14/157507/. Cited\n28 Nov 2022.\n32. Alexander N (2021) Obsolescence, forgotten: “Survivor Holograms”, virtual reality, and\nthe future of Holocaust commemoration. Cinergie-Il Cinema e le altre Arti 10(19):57–68.\n33. Manca S (2021) Digital memory in the post-witness era: how Holocaust museums use\nsocial media as new memory ecologies. Inf 12(1):1–17.\n34. Łysak T (2022) Vlogging Auschwitz: new players in Holocaust commemoration.\nHolocaust Stud 28(3):377–402.\n35. Divon T, Ebbrecht-Hartmann T (2022) JewishTikTok: The JewToks’ Fight against\nAntisemitism. In: TikTok cultures in the United States. Routledge, London.\n36. Makhortykh M, Urman A, Ulloa R (2021) Hey, Google, is it what the Holocaust looked\nlike? First Monday 26(10). https://doi.org/10.5210/fm.v26i10.11562.\n37. Wight AC (2020) Visitor perceptions of European Holocaust Heritage: a social media\nanalysis. Tour Manag 81:1–12.\n38. Kansteiner W (2017) The Holocaust in the 21st century: digital anxiety, transnational\ncosmopolitanism, and never again genocide without memory. In: Digital memory studies.\nRoutledge, London.\n39. Manca S, Passarelli M, Rehm M (2022) Exploring tensions in Holocaust museums’\nmodes of commemoration and interaction on social media. Technol Soc 68:1–13.\n16\n40. Guhl J, Davey J (2020) Hosting the ‘Holohoax’: a snapshot of Holocaust denial across\nsocial media. Available via The Institute for Strategic Dialogue. https://cutt.ly/G1bxtsR. Cited\n28 Nov 2022.\n41. Khumalo NB (2019) Silenced genocide voices in Zimbabwe’s archives: drawing lessons\nfrom\nRwanda’s\npost-genocide\narchives\nand\ndocumentation\ninitiatives.\nInf\nDev\n35(5):795–805.\n42. Hawkes M (2012) Containing testimony: Archiving loss after Genocide. Contin 26(6):\n935–945.\n43. Anabo IF, Elexpuru-Albizuri I, Villardón-Gallego L (2019) Revisiting the Belmont Report’s\nethical principles in internet-mediated research: perspectives from disciplinary associations\nin the social sciences. Ethics Inf Technol 21(2):137–149.\n44. Fujii LA (2010) Shades of truth and lies: interpreting testimonies of war and violence. J\nPeace Res 47(2):231–241.\n45. Hirsch M (2001) Surviving images: Holocaust photographs and the work of postmemory.\nYale J Crit 14(1):5–37.\n46. Jacobs JL (2004)Women, genocide, and memory: the ethics of feminist ethnography in\nHolocaust research. Gend & Soc 18(2):223–238.\n47. Luft A (2020) How do you repair a broken world? Conflict (ing) archives after the\nHolocaust. Qual Sociol 43(3):317–343.\n48. Crane SA (2008) Choosing not to look: representation, repatriation, and holocaust\natrocity photography. Hist Theory 47(3):309–330.\n49. Lang B (2000) Holocaust representation: art within the limits of history and ethics. JHU\nPress, Baltimore.\n50. Atkins SE (2009) Holocaust denial as an international movement. ABC-CLIO, Santa\nBarbara.\n51. Bastian M, Makhortykh M, Dobber T (2019) News personalization for peace: how\nalgorithmic\nrecommendations\ncan\nimpact\nconflict\ncoverage.\nInt\nJ\nConfl\nManag\n30(3):309–328.\n52. Storms E, Alvarado O, Monteiro-Krebs L (2022) ’Transparency is Meant for Control’ and\nVice Versa: learning from Co-designing and Evaluating Algorithmic News Recommenders.\nProc ACM Hum Comput Interact 6(CSCW2):1–24.\n53. OECD (2020) Combatting COVID-19 disinformation on online platforms. Available via\nOECD. https://cutt.ly/O1bYUGJ. Cited 28 Nov 2022.\n54. Assmann A (2010) The Holocaust—A global memory? Extensions and limits of a new\nmemory community. In Memory in a global age. Palgrave Macmillan, London.\n55. Jonassohn K (1998) Genocide and gross human rights violations: in comparative\nperspective. Transaction Publishers, Piscataway.\n17\n56. David L (2017) Against standardization of memory. Hum Rights Q 39(2):296–318.\n57. Dubey I (2021) Remembering, forgetting and memorialising: 1947, 1971 and the state of\nmemory studies in South Asia. India Rev 20(5):510–539\n58. Esposito E (2017) Algorithmic memory and the right to be forgotten on the web. Big Data\n& Soc 4(1):1–11.\n59. Makhortykh M (2021) Memoriae ex machina: how algorithms make us remember and\nforget. Georg J Int Aff 22(2):180–185.\n60. Ozbay FA, Alatas B (2020) Fake news detection within online social media using\nsupervised artificial intelligence algorithms. Phys Stat Mech Appl 540:1–19.\n61. Fernández-Pichel M, Losada DE, Pichel JC (2022) A multistage retrieval system for\nhealth-related misinformation detection. Eng Appl Artif Intell 115:105211.\n62.\nHelberger\nN\n(2019)\nOn\nthe\ndemocratic\nrole\nof\nnews\nrecommenders. Digit J\n7(8):993–1012.\n63. Curzon J,Kosa TA,Akalu R, El-KhatibK (2021) Privacy and artificial intelligence. IEEE\nTrans Artif Intell 2(2):96–108.\n"
    }
]